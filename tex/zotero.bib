
@inproceedings{reed_distance_2010,
	address = {New York, NY, USA},
	series = {{ICFP} '10},
	title = {Distance makes the types grow stronger: a calculus for differential privacy},
	isbn = {978-1-60558-794-3},
	url = {https://doi.org/10.1145/1863543.1863568},
	doi = {10.1145/1863543.1863568},
	abstract = {We want assurances that sensitive information will not be disclosed when aggregate data derived from a database is published. Differential privacy offers a strong statistical guarantee that the effect of the presence of any individual in a database will be negligible, even when an adversary has auxiliary knowledge. Much of the prior work in this area consists of proving algorithms to be differentially private one at a time; we propose to streamline this process with a functional language whose type system automatically guarantees differential privacy, allowing the programmer to write complex privacy-safe query programs in a ﬂexible and compositional way.},
	language = {en},
	booktitle = {Proceedings of the 15th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {Association for Computing Machinery},
	author = {Reed, Jason and Pierce, Benjamin C},
	year = {2010},
	pages = {12},
}

@article{bartheDecidingAccuracyDifferential2020,
	title = {Deciding {Accuracy} of {Differential} {Privacy} {Schemes}},
	volume = {5},
	url = {https://dl.acm.org/doi/10.1145/3434289},
	doi = {10.1145/3434289},
	abstract = {GILLES BARTHE, Max Planck Institute for Security and Privacy, Bochum, Germany ROHIT CHADHA, University of Missouri, USA PAUL KROGMEIER, University of Illinois, Urbana-Champaign, USA A. PRASAD SISTLA, University of Illinois, Chicago, USA MAHESH VISWANATHAN, University of Illinois, Urbana Champaign, USA Diﬀerential privacy is a mathematical framework for developing statistical computations with provable guarantees of privacy and accuracy. In contrast to the privacy component of diﬀerential privacy, which has a clear mathematical and intuitive meaning, the accuracy component of diﬀerential privacy does not have a general accepted deﬁnition; accuracy claims of diﬀerential privacy algorithms vary from algorithm to algorithm and are not instantiations of a general deﬁnition. We identify program discontinuity as a common theme in existing ad hoc deﬁnitions and introduce an alternative notion of accuracy parametrized by, what we call, distance to disagreement — the distance to disagreement of an input w.r.t. a deterministic computation and a distance , is the minimal distance ( , ) over all such that ( ) ≠ ( ). We show that our notion of accuracy subsumes the deﬁnition used in theoretical computer science, and captures known accuracy claims for diﬀerential privacy algorithms. In fact, our general notion of accuracy helps us prove better claims in some cases. Next, we study the decidability of accuracy. We ﬁrst show that accuracy is in general undecidable. Then, we deﬁne a non-trivial class of probabilistic computations for which accuracy is decidable (unconditionally, or assuming Schanuel’s conjecture). We implement our decision procedure and experimentally evaluate the eﬀectiveness of our approach for generating proofs or counterexamples of accuracy for common algorithms from the literature. CCS Concepts: • Security and privacy → Logic and veriﬁcation; • Software and its engineering → Formal software veriﬁcation.},
	language = {en},
	urldate = {2020-12-30},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Barthe, Gilles and Chadha, Rohit and Krogmeier, Paul and Sistla, A. Prasad and Viswanathan, Mahesh},
	month = nov,
	year = {2020},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Programming Languages, F.3.1},
}

@article{barthe_proving_2016,
	title = {Proving {Differential} {Privacy} via {Probabilistic} {Couplings}},
	url = {https://doi.org/10.1145/2933575.2934554},
	doi = {10.1145/2933575.2934554},
	abstract = {Over the last decade, differential privacy has achieved widespread adoption within the privacy community. Moreover, it has attracted signiﬁcant attention from the veriﬁcation community, resulting in several successful tools for formally proving differential privacy. Although their technical approaches vary greatly, all existing tools rely on reasoning principles derived from the composition theorem of differential privacy. While this sufﬁces to verify most common private algorithms, there are several important algorithms whose privacy analysis does not rely solely on the composition theorem. Their proofs are signiﬁcantly more complex, and are currently beyond the reach of veriﬁcation tools. In this paper, we develop compositional methods for formally verifying differential privacy for algorithms whose analysis goes beyond the composition theorem. Our methods are based on deep connections between differential privacy and probabilistic couplings, an established mathematical tool for reasoning about stochastic processes. Even when the composition theorem is not helpful, we can often prove privacy by a coupling argument.},
	language = {en},
	urldate = {2021-01-10},
	journal = {Proceedings of the 31st Annual ACM/IEEE Symposium on Logic in Computer Science},
	author = {Barthe, Gilles and Gaboardi, Marco and Grégoire, Benjamin and Hsu, Justin and Strub, Pierre-Yves},
	month = jul,
	year = {2016},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Data Structures and Algorithms, Computer Science - Logic in Computer Science, F.3.1},
	pages = {749--758},
}

@inproceedings{barthe_advanced_2016,
	address = {Vienna Austria},
	title = {Advanced {Probabilistic} {Couplings} for {Differential} {Privacy}},
	isbn = {978-1-4503-4139-4},
	url = {https://dl.acm.org/doi/10.1145/2976749.2978391},
	doi = {10.1145/2976749.2978391},
	abstract = {Differential privacy is a promising formal approach to data privacy, which provides a quantitative bound on the privacy cost of an algorithm that operates on sensitive information. Several tools have been developed for the formal veriﬁcation of differentially private algorithms, including program logics and type systems. However, these tools do not capture fundamental techniques that have emerged in recent years, and cannot be used for reasoning about cutting-edge differentially private algorithms. Existing techniques fail to handle three broad classes of algorithms: 1) algorithms where privacy depends on accuracy guarantees, 2) algorithms that are analyzed with the advanced composition theorem, which shows slower growth in the privacy cost, 3) algorithms that interactively accept adaptive inputs.},
	language = {en},
	urldate = {2021-01-10},
	booktitle = {Proceedings of the 2016 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Barthe, Gilles and Fong, Noémie and Gaboardi, Marco and Grégoire, Benjamin and Hsu, Justin and Strub, Pierre-Yves},
	month = oct,
	year = {2016},
	pages = {55--67},
}

@inproceedings{zhang_lightdp_2017,
	address = {Paris, France},
	series = {{POPL}},
	title = {{LightDP}: {Towards} {Automating} {Differential} {Privacy} {Proofs}},
	isbn = {978-1-4503-4660-3},
	shorttitle = {{LightDP}},
	url = {https://doi.org/10.1145/3009837.3009884},
	doi = {10.1145/3009837.3009884},
	abstract = {The growing popularity and adoption of differential privacy in academic and industrial settings has resulted in the development of increasingly sophisticated algorithms for releasing information while preserving privacy. Accompanying this phenomenon is the natural rise in the development and publication of incorrect algorithms, thus demonstrating the necessity of formal veriﬁcation tools. However, existing formal methods for differential privacy face a dilemma: methods based on customized logics can verify sophisticated algorithms but come with a steep learning curve and signiﬁcant annotation burden on the programmers, while existing programming platforms lack expressive power for some sophisticated algorithms.},
	language = {en},
	urldate = {2021-01-02},
	booktitle = {Proceedings of the 44th {ACM} {SIGPLAN} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Danfeng and Kifer, Daniel},
	month = jan,
	year = {2017},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Programming Languages, D.2.4, D.3.1, F.3.1},
	pages = {888--901},
}

@article{barthe_probabilistic_2013,
	title = {Probabilistic {Relational} {Reasoning} for {Differential} {Privacy}},
	volume = {35},
	url = {https://doi.org/10.1145/2492061},
	doi = {10.1145/2492061},
	language = {en},
	number = {3},
	journal = {ACM Transactions on Programming Languages and Systems},
	author = {Barthe, Gilles and Pf, Boris Ko and Olmedo, Federico and Guelin, Santiago Zanella-Be},
	month = nov,
	year = {2013},
	pages = {46},
}

@inproceedings{barthe_higher-order_2015,
	address = {Mumbai, India},
	series = {{POPL} ’15},
	title = {Higher-{Order} {Approximate} {Relational} {Reﬁnement} {Types} for {Mechanism} {Design} and {Differential} {Privacy}},
	volume = {50},
	isbn = {978-1-4503-3300-9},
	url = {https://doi.org/10.1145/2676726.2677000},
	doi = {10.1145/2775051.2677000},
	abstract = {Mechanism design is the study of algorithm design where the inputs to the algorithm are controlled by strategic agents, who must be incentivized to faithfully report them. Unlike typical programmatic properties, it is not sufﬁcient for algorithms to merely satisfy the property—incentive properties are only useful if the strategic agents also believe this fact.},
	language = {en},
	booktitle = {roceedings of the 42nd {Annual} {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {Association for Computing Machinery},
	author = {Barthe, Gilles and Gaboardi, Marco and Arias, Emilio Jesús Gallego and Hsu, Justin and Roth, Aaron and Strub, Pierre-Yves},
	year = {2015},
	pages = {14},
}

@inproceedings{haeberlen_differential_2011,
	address = {USA},
	series = {{SEC}'11},
	title = {Differential {Privacy} {Under} {Fire}},
	url = {https://dl.acm.org/doi/10.5555/2028067.2028100},
	abstract = {Anonymizing private data before release is not enough to reliably protect privacy, as Netﬂix and AOL have learned to their cost. Recent research on differential privacy opens a way to obtain robust, provable privacy guarantees, and systems like PINQ and Airavat now offer convenient frameworks for processing arbitrary userspeciﬁed queries in a differentially private way. However, these systems are vulnerable to a variety of covertchannel attacks that can be exploited by an adversarial querier.},
	language = {en},
	booktitle = {Proceedings of the 20th {USENIX} {Conference} on {Security}},
	publisher = {USENIX Association},
	author = {Haeberlen, Andreas and Pierce, Benjamin C and Narayan, Arjun},
	year = {2011},
	pages = {33},
}

@inproceedings{bartheDecidingDifferentialPrivacy2020,
	address = {Saarbrücken Germany},
	title = {Deciding {Differential} {Privacy} for {Programs} with {Finite} {Inputs} and {Outputs}},
	isbn = {978-1-4503-7104-9},
	url = {https://dl.acm.org/doi/10.1145/3373718.3394796},
	doi = {10.1145/3373718.3394796},
	abstract = {Differential privacy is a de facto standard for statistical computations over databases that contain private data. Its main and rather surprising strength is to guarantee individual privacy and yet allow for accurate statistical results. Thanks to its mathematical definition, differential privacy is also a natural target for formal analysis. A broad line of work develops and uses logical methods for proving privacy. A more recent and complementary line of work uses statistical methods for finding privacy violations. Although both lines of work are practically successful, they elide the fundamental question of decidability.},
	language = {en},
	urldate = {2020-12-28},
	booktitle = {Proceedings of the 35th {Annual} {ACM}/{IEEE} {Symposium} on {Logic} in {Computer} {Science}},
	publisher = {ACM},
	author = {Barthe, Gilles and Chadha, Rohit and Jagannath, Vishal and Sistla, A. Prasad and Viswanathan, Mahesh},
	month = jul,
	year = {2020},
	pages = {141--154},
}

@article{dwork_calibrating_2006,
	title = {Calibrating {Noise} to {Sensitivity} in {Private} {Data} {Analysis}},
	volume = {7},
	url = {https://dx.doi.org/10.1007/11681878_14},
	doi = {10.1007/11681878_14},
	language = {en},
	journal = {Theory of Cryptography},
	author = {Dwork, Cynthia and McSherry, Frank and Nissim, Kobbi and Smith, Adam},
	year = {2006},
	pages = {36},
}

@article{kleinberg_week_nodate,
	title = {{WEEK} 5: {TEXT} {MINING} 3 {SECU0057}},
	language = {en},
	author = {Kleinberg, Bennett},
	pages = {65},
}

@article{thomas_framing_nodate,
	title = {Framing {Dependencies} {Introduced} by {Underground} {Commoditization}},
	abstract = {Internet crime has become increasingly dependent on the underground economy: a loose federation of specialists selling capabilities, services, and resources explicitly tailored to the abuse ecosystem. Through these emerging markets, modern criminal entrepreneurs piece together dozens of à la carte components into entirely new criminal endeavors. From an abuse ﬁghting perspective, criminal reliance on this black market introduces fragile dependencies that, if disrupted, undermine entire operations that as a composite appear intractable to protect against. However, without a clear framework for examining the costs and infrastructure behind Internet crime, it becomes impossible to evaluate the effectiveness of novel intervention strategies.},
	language = {en},
	author = {Thomas, Kurt and Huang, Danny Yuxing and Wang, David and Bursztein, Elie and Grier, Chris and Holt, Thomas J and Kruegel, Christopher and McCoy, Damon and Savage, Stefan and Vigna, Giovanni},
	pages = {24},
}

@misc{the_capitol_forum_lina_2018,
	title = {Lina {Khan}, {Open} {Markets} {Institute}},
	url = {https://www.youtube.com/watch?v=yFnzKixEj98},
	urldate = {2021-02-28},
	author = {{The Capitol Forum}},
	month = may,
	year = {2018},
}

@article{kleinberg_week_nodate-1,
	title = {{WEEK} 4: {TEXT} {MINING} 2 {SECU0057}},
	language = {en},
	author = {Kleinberg, Bennett},
	pages = {75},
}

@inproceedings{passi_problem_2019,
	address = {Atlanta GA USA},
	title = {Problem {Formulation} and {Fairness}},
	isbn = {978-1-4503-6125-5},
	url = {https://dl.acm.org/doi/10.1145/3287560.3287567},
	doi = {10.1145/3287560.3287567},
	abstract = {Formulating data science problems is an uncertain and difficult process. It requires various forms of discretionary work to translate high-level objectives or strategic goals into tractable problems, necessitating, among other things, the identification of appropriate target variables and proxies. While these choices are rarely self-evident, normative assessments of data science projects often take them for granted, even though different translations can raise profoundly different ethical concerns. Whether we consider a data science project fair often has as much to do with the formulation of the problem as any property of the resulting model. Building on six months of ethnographic fieldwork with a corporate data science team—and channeling ideas from sociology and history of science, critical data studies, and early writing on knowledge discovery in databases—we describe the complex set of actors and activities involved in problem formulation. Our research demonstrates that the specification and operationalization of the problem are always negotiated and elastic, and rarely worked out with explicit normative considerations in mind. In so doing, we show that careful accounts of everyday data science work can help us better understand how and why data science problems are posed in certain ways—and why specific formulations prevail in practice, even in the face of what might seem like normatively preferable alternatives. We conclude by discussing the implications of our findings, arguing that effective normative interventions will require attending to the practical work of problem formulation.},
	language = {en},
	urldate = {2021-02-24},
	booktitle = {Proceedings of the {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Passi, Samir and Barocas, Solon},
	month = jan,
	year = {2019},
	pages = {39--48},
}

@article{black_regulating_2019,
	title = {Regulating {AI} and machine learning: setting the regulatory agenda},
	volume = {10},
	abstract = {Disruptive technologies arrive with regularity. Whether it is the first industrial revolution with steam powered factories and transportation, or subsequent revolutions which brought about chemical engineering, communications revolutions, aviation and eventually biotechnology and digitisation. We stand at the edge of the next revolution the AI revolution where methods of artificial intelligence and machine learning offer possibilities hitherto unimagined. How this revolution develops and how our society absorbs the potential of this new technology will be largely determined by the models of regulation and governance applied to the nascent technology. In this paper the authors examine lessons from history and propose a framework for identifying and analysing the key elements of regulatory regimes and their interactions which can form the basis for developing a new model for AI regulatory systems. Furthermore, it argues that the goals of such systems should be to manage the risks different models and uses of AI pose, not just the ethical issues they create.},
	language = {en},
	number = {3},
	author = {Black, Julia and Murray, Andrew D},
	year = {2019},
	pages = {22},
}

@article{mittelstadt_principles_2019,
	title = {Principles alone cannot guarantee ethical {AI}},
	volume = {1},
	issn = {2522-5839},
	url = {http://www.nature.com/articles/s42256-019-0114-4},
	doi = {10.1038/s42256-019-0114-4},
	language = {en},
	number = {11},
	urldate = {2021-02-24},
	journal = {Nature Machine Intelligence},
	author = {Mittelstadt, Brent},
	month = nov,
	year = {2019},
	pages = {501--507},
}

@inproceedings{christin_traveling_2013,
	address = {Rio de Janeiro, Brazil},
	title = {Traveling the silk road: a measurement analysis of a large anonymous online marketplace},
	isbn = {978-1-4503-2035-1},
	shorttitle = {Traveling the silk road},
	url = {http://dl.acm.org/citation.cfm?doid=2488388.2488408},
	doi = {10.1145/2488388.2488408},
	abstract = {We perform a comprehensive measurement analysis of Silk Road, an anonymous, international online marketplace that operates as a Tor hidden service and uses Bitcoin as its exchange currency. We gather and analyze data over eight months between the end of 2011 and 2012, including daily crawls of the marketplace for nearly six months in 2012. We obtain a detailed picture of the type of goods sold on Silk Road, and of the revenues made both by sellers and Silk Road operators. Through examining over 24,400 separate items sold on the site, we show that Silk Road is overwhelmingly used as a market for controlled substances and narcotics, and that most items sold are available for less than three weeks. The majority of sellers disappears within roughly three months of their arrival, but a core of 112 sellers has been present throughout our measurement interval. We evaluate the total revenue made by all sellers, from public listings, to slightly over USD 1.2 million per month; this corresponds to about USD 92,000 per month in commissions for the Silk Road operators. We further show that the marketplace has been operating steadily, with daily sales and number of sellers overall increasing over our measurement interval. We discuss economic and policy implications of our analysis and results, including ethical considerations for future research in this area.},
	language = {en},
	urldate = {2021-02-24},
	booktitle = {Proceedings of the 22nd international conference on {World} {Wide} {Web} - {WWW} '13},
	publisher = {ACM Press},
	author = {Christin, Nicolas},
	year = {2013},
	pages = {213--224},
}

@inproceedings{levchenko_click_2011,
	address = {Berkeley, CA},
	title = {Click {Trajectories}: {End}-to-{End} {Analysis} of the {Spam} {Value} {Chain}},
	isbn = {978-0-7695-4402-1 978-1-4577-0147-4},
	shorttitle = {Click {Trajectories}},
	url = {http://ieeexplore.ieee.org/document/5958044/},
	doi = {10.1109/SP.2011.24},
	abstract = {Spam-based advertising is a business. While it has engendered both widespread antipathy and a multi-billion dollar anti-spam industry, it continues to exist because it fuels a proﬁtable enterprise. We lack, however, a solid understanding of this enterprise’s full structure, and thus most anti-spam interventions focus on only one facet of the overall spam value chain (e.g., spam ﬁltering, URL blacklisting, site takedown). In this paper we present a holistic analysis that quantiﬁes the full set of resources employed to monetize spam email—including naming, hosting, payment and fulﬁllment—using extensive measurements of three months of diverse spam data, broad crawling of naming and hosting infrastructures, and over 100 purchases from spam-advertised sites. We relate these resources to the organizations who administer them and then use this data to characterize the relative prospects for defensive interventions at each link in the spam value chain. In particular, we provide the ﬁrst strong evidence of payment bottlenecks in the spam value chain; 95\% of spam-advertised pharmaceutical, replica and software products are monetized using merchant services from just a handful of banks.},
	language = {en},
	urldate = {2021-02-24},
	booktitle = {2011 {IEEE} {Symposium} on {Security} and {Privacy}},
	publisher = {IEEE},
	author = {Levchenko, K. and Pitsillidis, A. and Chachra, N. and Enright, B. and Felegyhazi, M. and Grier, C. and Halvorson, T. and Kanich, C. and Kreibich, C. and {He Liu} and McCoy, D. and Weaver, N. and Paxson, V. and Voelker, G. M. and Savage, S.},
	month = may,
	year = {2011},
	pages = {431--446},
}

@inproceedings{stone-gross_your_2009,
	address = {Chicago, Illinois, USA},
	title = {Your botnet is my botnet: analysis of a botnet takeover},
	isbn = {978-1-60558-894-0},
	shorttitle = {Your botnet is my botnet},
	url = {http://portal.acm.org/citation.cfm?doid=1653662.1653738},
	doi = {10.1145/1653662.1653738},
	abstract = {Botnets, networks of malware-infected machines that are controlled by an adversary, are the root cause of a large number of security problems on the Internet. A particularly sophisticated and insidious type of bot is Torpig, a malware program that is designed to harvest sensitive information (such as bank account and credit card data) from its victims. In this paper, we report on our efforts to take control of the Torpig botnet and study its operations for a period of ten days. During this time, we observed more than 180 thousand infections and recorded almost 70 GB of data that the bots collected. While botnets have been “hijacked” and studied previously, the Torpig botnet exhibits certain properties that make the analysis of the data particularly interesting. First, it is possible (with reasonable accuracy) to identify unique bot infections and relate that number to the more than 1.2 million IP addresses that contacted our command and control server. Second, the Torpig botnet is large, targets a variety of applications, and gathers a rich and diverse set of data from the infected victims. This data provides a new understanding of the type and amount of personal information that is stolen by botnets.},
	language = {en},
	urldate = {2021-02-24},
	booktitle = {Proceedings of the 16th {ACM} conference on {Computer} and communications security - {CCS} '09},
	publisher = {ACM Press},
	author = {Stone-Gross, Brett and Cova, Marco and Cavallaro, Lorenzo and Gilbert, Bob and Szydlowski, Martin and Kemmerer, Richard and Kruegel, Christopher and Vigna, Giovanni},
	year = {2009},
	pages = {635},
}

@article{mittelstadt_principles_2019-1,
	title = {Principles alone cannot guarantee ethical {AI}},
	volume = {1},
	issn = {2522-5839},
	url = {http://www.nature.com/articles/s42256-019-0114-4},
	doi = {10.1038/s42256-019-0114-4},
	language = {en},
	number = {11},
	urldate = {2021-02-21},
	journal = {Nature Machine Intelligence},
	author = {Mittelstadt, Brent},
	month = nov,
	year = {2019},
	pages = {501--507},
}

@inproceedings{papernot_sok_2018,
	address = {London},
	title = {{SoK}: {Security} and {Privacy} in {Machine} {Learning}},
	isbn = {978-1-5386-4228-3},
	shorttitle = {{SoK}},
	url = {https://ieeexplore.ieee.org/document/8406613/},
	doi = {10.1109/EuroSP.2018.00035},
	abstract = {Advances in machine learning (ML) in recent years have enabled a dizzying array of applications such as data analytics, autonomous systems, and security diagnostics. ML is now pervasive—new systems and models are being deployed in every domain imaginable, leading to widespread deployment of software based inference and decision making. There is growing recognition that ML exposes new vulnerabilities in software systems, yet the technical community’s understanding of the nature and extent of these vulnerabilities remains limited. We systematize ﬁndings on ML security and privacy, focusing on attacks identiﬁed on these systems and defenses crafted to date. We articulate a comprehensive threat model for ML, and categorize attacks and defenses within an adversarial framework. Key insights resulting from works both in the ML and security communities are identiﬁed and the effectiveness of approaches are related to structural elements of ML algorithms and the data used to train them. In particular, it is apparent that constructing a theoretical understanding of the sensitivity of modern ML algorithms to the data they analyze, a` la PAC theory, will foster a science of security and privacy in ML.},
	language = {en},
	urldate = {2021-02-17},
	booktitle = {2018 {IEEE} {European} {Symposium} on {Security} and {Privacy} ({EuroS}\&{P})},
	publisher = {IEEE},
	author = {Papernot, Nicolas and McDaniel, Patrick and Sinha, Arunesh and Wellman, Michael P.},
	month = apr,
	year = {2018},
	pages = {399--414},
}

@article{sharad_-anonymizing_nodate,
	title = {De-anonymizing {D4D} {Datasets}},
	abstract = {Recent research on de-anonymizing datasets of anonymized personal records has not deterred organizations from releasing personal data, often with ingenuous attempts at defeating de-anonymization. Studying such techniques provides scientiﬁc evidence as to why anonymization of high dimensional databases is hard and throws light on what kinds of techniques to avoid. We study how to de-anonymize datasets released as a part of Data for Development (D4D) challenge [12]. We show that the anonymization strategy used is weak and allows an attacker to re-identify and link records eﬃciently, we also suggest some measures to make such attacks harder.},
	language = {en},
	author = {Sharad, Kumar and Danezis, George},
	pages = {17},
}

@misc{ahmad_sultan_improving_2019,
	title = {Improving {Cybersecurity} {Awareness} in {Underserved} {Populations}},
	url = {https://cltc.berkeley.edu/2019/04/15/improving-cybersecurity-awareness-in-underserved-populations/},
	publisher = {Centre for Long-Term Cybersecurity},
	author = {{Ahmad Sultan}},
	month = apr,
	year = {2019},
}

@book{menezes_handbook_1997,
	address = {Boca Raton},
	series = {{CRC} {Press} series on discrete mathematics and its applications},
	title = {Handbook of applied cryptography},
	isbn = {978-0-8493-8523-0},
	language = {en},
	publisher = {CRC Press},
	author = {Menezes, A. J. and Van Oorschot, Paul C. and Vanstone, Scott A.},
	year = {1997},
	keywords = {Access control Handbooks, manuals, etc, Computers, Cryptography, Handbooks, manuals, etc},
}

@article{lauritsen_expanding_2017,
	title = {Expanding {Our} {Understanding} of {Crime}: {The} {National} {Academies} {Report} on the {Future} of {Crime} {Statistics} and {Measurement}},
	volume = {16},
	issn = {15386473},
	shorttitle = {Expanding {Our} {Understanding} of {Crime}},
	url = {http://doi.wiley.com/10.1111/1745-9133.12332},
	doi = {10.1111/1745-9133.12332},
	language = {en},
	number = {4},
	urldate = {2021-02-17},
	journal = {Criminology \& Public Policy},
	author = {Lauritsen, Janet L. and Cork, Daniel L.},
	month = nov,
	year = {2017},
	pages = {1075--1098},
}

@article{steenmans_analysing_nodate,
	title = {Analysing {Network} {Structure} \& {Behaviour} {STEP0020} - {Analytic} {Methods} for {Policy}},
	language = {en},
	author = {Steenmans, Ine and Veale, Michael},
	pages = {8},
}

@article{tandoc_defining_2018,
	title = {Defining “{Fake} {News}”: {A} typology of scholarly definitions},
	volume = {6},
	issn = {2167-0811, 2167-082X},
	shorttitle = {Defining “{Fake} {News}”},
	url = {https://www.tandfonline.com/doi/full/10.1080/21670811.2017.1360143},
	doi = {10.1080/21670811.2017.1360143},
	language = {en},
	number = {2},
	urldate = {2021-01-31},
	journal = {Digital Journalism},
	author = {Tandoc, Edson C. and Lim, Zheng Wei and Ling, Richard},
	month = feb,
	year = {2018},
	pages = {137--153},
}

@article{ferrara_rise_2016,
	title = {The rise of social bots},
	volume = {59},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2818717},
	doi = {10.1145/2818717},
	language = {en},
	number = {7},
	urldate = {2021-01-31},
	journal = {Communications of the ACM},
	author = {Ferrara, Emilio and Varol, Onur and Davis, Clayton and Menczer, Filippo and Flammini, Alessandro},
	month = jun,
	year = {2016},
	pages = {96--104},
}

@article{lazer_science_2018,
	title = {The science of fake news},
	volume = {359},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.aao2998},
	doi = {10.1126/science.aao2998},
	language = {en},
	number = {6380},
	urldate = {2021-01-31},
	journal = {Science},
	author = {Lazer, David M. J. and Baum, Matthew A. and Benkler, Yochai and Berinsky, Adam J. and Greenhill, Kelly M. and Menczer, Filippo and Metzger, Miriam J. and Nyhan, Brendan and Pennycook, Gordon and Rothschild, David and Schudson, Michael and Sloman, Steven A. and Sunstein, Cass R. and Thorson, Emily A. and Watts, Duncan J. and Zittrain, Jonathan L.},
	month = mar,
	year = {2018},
	pages = {1094--1096},
}

@article{steenmans_step0020_nodate,
	title = {{STEP0020} - {Describing} {A} {Policy} {System}'s {Behaviour} with {Statistics}},
	language = {en},
	author = {Steenmans, Ine and Veale, Michael},
	pages = {6},
}

@article{cohen_towards_2020,
	title = {Towards formalizing the {GDPR}’s notion of singling out},
	volume = {117},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1914598117},
	doi = {10.1073/pnas.1914598117},
	abstract = {There is a significant conceptual gap between legal and mathematical thinking around data privacy. The effect is uncertainty as to which technical offerings meet legal standards. This uncertainty is exacerbated by a litany of successful privacy attacks demonstrating that traditional statistical disclosure limitation techniques often fall short of the privacy envisioned by regulators. We define “predicate singling out,” a type of privacy attack intended to capture the concept of singling out appearing in the General Data Protection Regulation (GDPR). An adversary predicate singles out a dataset x using the output of a data-release mechanism
              
                
                  M
                  
                    (
                    
                      x
                    
                    )
                  
                
              
              if it finds a predicate p matching exactly one row in x with probability much better than a statistical baseline. A data-release mechanism that precludes such attacks is “secure against predicate singling out” (
              PSO secure
              ). We argue that PSO security is a mathematical concept with legal consequences. Any data-release mechanism that purports to “render anonymous” personal data under the GDPR must prevent singling out and, hence, must be PSO secure. We analyze the properties of PSO security, showing that it fails to compose. Namely, a combination of more than logarithmically many exact counts, each individually PSO secure, facilitates predicate singling out. Finally, we ask whether differential privacy and k-anonymity are PSO secure. Leveraging a connection to statistical generalization, we show that differential privacy implies PSO security. However, and in contrast with current legal guidance, k-anonymity does not: There exists a simple predicate singling out attack under mild assumptions on the k-anonymizer and the data distribution.},
	language = {en},
	number = {15},
	urldate = {2021-01-27},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Cohen, Aloni and Nissim, Kobbi},
	month = apr,
	year = {2020},
	pages = {8344--8352},
}

@article{kleinberg_week_nodate-2,
	title = {{WEEK} 3: {TEXT} {MINING} {I} {SECU0057}},
	language = {en},
	author = {Kleinberg, Bennett},
	pages = {75},
}

@inproceedings{abu_rajab_multifaceted_2006,
	address = {Rio de Janeriro, Brazil},
	title = {A multifaceted approach to understanding the botnet phenomenon},
	isbn = {978-1-59593-561-8},
	url = {http://portal.acm.org/citation.cfm?doid=1177080.1177086},
	doi = {10.1145/1177080.1177086},
	abstract = {The academic community has long acknowledged the existence of malicious botnets, however to date, very little is known about the behavior of these distributed computing platforms. To the best of our knowledge, botnet behavior has never been methodically studied, botnet prevalence on the Internet is mostly a mystery, and the botnet life cycle has yet to be modeled. Uncertainty abounds. In this paper, we attempt to clear the fog surrounding botnets by constructing a multifaceted and distributed measurement infrastructure. Throughout a period of more than three months, we used this infrastructure to track 192 unique IRC botnets of size ranging from a few hundred to several thousand infected end-hosts. Our results show that botnets represent a major contributor to unwanted Internet trafﬁc—27\% of all malicious connection attempts observed from our distributed darknet can be directly attributed to botnetrelated spreading activity. Furthermore, we discovered evidence of botnet infections in 11\% of the 800,000 DNS domains we examined, indicating a high diversity among botnet victims. Taken as a whole, these results not only highlight the prominence of botnets, but also provide deep insights that may facilitate further research to curtail this phenomenon.},
	language = {en},
	urldate = {2021-01-25},
	booktitle = {Proceedings of the 6th {ACM} {SIGCOMM} on {Internet} measurement  - {IMC} '06},
	publisher = {ACM Press},
	author = {Abu Rajab, Moheeb and Zarfoss, Jay and Monrose, Fabian and Terzis, Andreas},
	year = {2006},
	pages = {41},
}

@article{eden_problem_2013,
	title = {Problem structuring: on the nature of, and reaching agreement about, goals},
	volume = {1},
	issn = {2193-9438, 2193-9446},
	shorttitle = {Problem structuring},
	url = {http://link.springer.com/10.1007/s40070-013-0005-6},
	doi = {10.1007/s40070-013-0005-6},
	abstract = {In this paper, we raise issues about discovering and modelling purpose that, in our view, can often be missed within operational research practice. We suggest that, in problem solving, there is a danger of taking too little account of: the differences between espoused goals and goals-in-use; the potentially misleading nature of published goals; goals that express the need to avoid outcomes—‘negative goals’; the meaning of goals in an action context rather than the semantics of goal statements; the dynamics and clarity implied by goal relationships; the potential that derives from multi-organisational settings where goals that express an outcome that can only be achieved collaboratively; stakeholder responses to expressed goals—that good solutions can be sabotaged by others; the fact that some goals are contextually important but not a focus for problem solving because they are ‘notour-core-goals’; and the need to design ambiguity of purpose in expressing goals systems. These issues are illustrated through a number of real case examples drawn from engineering, Police, NHS, a Research Institute, and a Utility company/Regulator setting.},
	language = {en},
	number = {1-2},
	urldate = {2021-01-25},
	journal = {EURO Journal on Decision Processes},
	author = {Eden, Colin and Ackermann, Fran},
	month = jun,
	year = {2013},
	pages = {7--28},
}

@article{kifer_guidelines_2020,
	title = {Guidelines for {Implementing} and {Auditing} {Differentially} {Private} {Systems}},
	url = {http://arxiv.org/abs/2002.04049},
	abstract = {Diﬀerential privacy is an information theoretic constraint on algorithms and code. It provides quantiﬁcation of privacy leakage and formal privacy guarantees that are currently considered the gold standard in privacy protections. In this paper we provide an initial set of “best practices” for developing diﬀerentially private platforms, techniques for unit testing that are speciﬁc to diﬀerential privacy, guidelines for checking if diﬀerential privacy is being applied correctly in an application, and recommendations for parameter settings. The genesis of this paper was an initiative by Facebook and Social Science One to provide social science researchers with programmatic access to a URL-shares dataset. In order to maximize the utility of the data for research while protecting privacy, researchers should access the data through an interactive platform that supports diﬀerential privacy.},
	language = {en},
	urldate = {2021-01-25},
	journal = {arXiv:2002.04049 [cs]},
	author = {Kifer, Daniel and Messing, Solomon and Roth, Aaron and Thakurta, Abhradeep and Zhang, Danfeng},
	month = may,
	year = {2020},
	note = {arXiv: 2002.04049},
	keywords = {Computer Science - Cryptography and Security},
}

@article{fuster_opening_2013,
	title = {Opening up personal data protection: {A} conceptual controversy},
	abstract = {The existence of a fundamental right to the protection of personal data in European Union (EU) law is nowadays undisputed. Established in the EU Charter of Fundamental Rights in 2000, it is increasingly permeating EU secondary law, and is expected to play a key role in the future EU personal data protection landscape. The right’s reinforced visibility has rendered manifest the co-existence of two possible and contrasting interpretations as to what it come to mean. If some envision it as a primarily permissive right, enabling the processing of such data under certain conditions, others picture it as having a prohibitive nature, implying that any processing of data is a limitation of the right, be it legitimate or illegitimate. This paper investigates existing tensions between different understandings of the right to the protection of personal data, and explores the assumptions and conceptual legacies underlying both approaches. It traces their historical lineages, and, focusing on the right to personal data protection as established by the EU Charter, analyses the different arguments that can ground contrasted readings of its Article 8. It also reviews the conceptualisations of personal data protection as present in the literature, and ﬁnally contrasts all these perspectives with the construal of the right by the EU Court of Justice.},
	language = {en},
	author = {Fuster, Gloria González},
	year = {2013},
	pages = {9},
}

@article{zang_who_nodate,
	title = {Who {Knows} {What} {About} {Me}? {A} {Survey} of {Behind} the {Scenes} {Personal} {Data} {Sharing} to {Third} {Parties} by {Mobile} {Apps}},
	abstract = {What types of user data are mobile apps sending to third parties? We chose 110 of the most popular free mobile apps as of June-July 2014 from the Google Play Store and Apple App Store, across 9 categories likely to handle potentially sensitive data about users including job information, medical data, and location. For each app, we used a man-in-the-middle proxy to record HTTP and HTTPS traffic that occurred while using the app and looked for transmissions that include personally identifiable information (PII), behavior data such as search terms, and location data, including geo-coordinates. An app that collects these data types may not need to notify the user in current permissions systems.},
	language = {en},
	author = {Zang, Jinyan and Dummit, Krysta and Graves, Jim and Lisker, Paul and Sweeney, Latanya},
	pages = {53},
}

@article{purtova_default_2014,
	title = {Default entitlements in personal data in the proposed {Regulation}: {Informational} self-determination off the table … and back on again?},
	volume = {30},
	issn = {02673649},
	shorttitle = {Default entitlements in personal data in the proposed {Regulation}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0267364913002069},
	doi = {10.1016/j.clsr.2013.12.006},
	abstract = {This paper aims to assess the proposed General Data Protection Regulation through the framework of default entitlements in personal data. The notion of default entitlements comes from economic analysis of law and provides new insights into the implications of the data protection reform. While, under the principle of informational self-determination the default entitlements should lie with the individual, the Commission is shown to assign a great deal of default rights to others, including the Information Industry. This article cautions against the possibility of reducing the European system of data protection rooted in the values of individual autonomy and informational self-determination to a mere set of administrative rules channelling the ﬂow of personal data, yet without a clear direction.},
	language = {en},
	number = {1},
	urldate = {2021-01-25},
	journal = {Computer Law \& Security Review},
	author = {Purtova, Nadezhda},
	month = feb,
	year = {2014},
	pages = {6--24},
}

@article{tikkinen-piri_eu_2018,
	title = {{EU} {General} {Data} {Protection} {Regulation}: {Changes} and implications for personal data collecting companies},
	volume = {34},
	issn = {02673649},
	shorttitle = {{EU} {General} {Data} {Protection} {Regulation}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0267364917301966},
	doi = {10.1016/j.clsr.2017.05.015},
	abstract = {The General Data Protection Regulation (GDPR) will come into force in the European Union (EU) in May 2018 to meet current challenges related to personal data protection and to harmonise data protection across the EU. Although the GDPR is anticipated to beneﬁt companies by offering consistency in data protection activities and liabilities across the EU countries and by enabling more integrated EU-wide data protection policies, it poses new challenges to companies. They are not necessarily prepared for the changes and may lack awareness of the upcoming requirements and the GDPR’s coercive measures. The implementation of the GDPR requirements demands substantial ﬁnancial and human resources, as well as training of employees; hence, companies need guidance to support them in this transition. The purposes of this study were to compare the current Data Protection Directive 95/46/EC with the GDPR by systematically analysing their differences and to identify the GDPR’s practical implications, speciﬁcally for companies that provide services based on personal data. This study aimed to identify and discuss the changes introduced by the GDPR that would have the most practical relevance to these companies and possibly affect their data management and usage practices. Therefore, a review and a thematic analysis and synthesis of the article-level changes were carried out. Through the analysis, the key practical implications of the changes were identiﬁed and classiﬁed. As a synthesis of the results, a framework was developed, presenting 12 aspects of these implications and the corresponding guidance on how to prepare for the new requirements. These aspects cover business strategies and practices, as well as organisational and technical measures.},
	language = {en},
	number = {1},
	urldate = {2021-01-25},
	journal = {Computer Law \& Security Review},
	author = {Tikkinen-Piri, Christina and Rohunen, Anna and Markkula, Jouni},
	month = feb,
	year = {2018},
	pages = {134--153},
}

@article{llanos_privacy_nodate,
	title = {Privacy is {Dead}! {Long} {Live} {Data} {Protection}…{The} {Appropriateness} of {Data} {Protection} {Laws}},
	language = {en},
	author = {Llanos, Dr Jose-Tomas},
	pages = {28},
}

@book{clough_principles_2010,
	address = {Cambridge},
	edition = {1},
	title = {Principles of {Cybercrime}},
	isbn = {978-1-139-54080-3},
	url = {http://ebooks.cambridge.org/ref/id/CBO9781139540803},
	urldate = {2021-01-20},
	publisher = {Cambridge University Press},
	author = {Clough, Jonathan},
	year = {2010},
	doi = {10.1017/CBO9781139540803},
}

@article{herley_why_nodate,
	title = {Why do {Nigerian} {Scammers} {Say} {They} are from {Nigeria}?},
	abstract = {False positives cause many promising detection technologies to be unworkable in practice. Attackers, we show, face this problem too. In deciding who to attack true positives are targets successfully attacked, while false positives are those that are attacked but yield nothing.},
	language = {en},
	author = {Herley, Cormac},
	pages = {14},
}

@article{becker_cybercrime_nodate,
	title = {Cybercrime: {Fraud}, {Identity} {Theft}, {Cyber} {Bullying}},
	language = {en},
	journal = {Identity Theft},
	author = {Becker, Ingolf},
	pages = {73},
}

@misc{richard_c_hollinger_hackers_nodate,
	title = {Hackers: {Computer} {Heroes} or {Electronic} {Highwaymen}},
	url = {https://dl-acm-org.libproxy.ucl.ac.uk/doi/pdf/10.1145/122246.122248},
	urldate = {2021-01-18},
	author = {{Richard C Hollinger}},
}

@article{becker_cybercrime_nodate-1,
	title = {Cybercrime: {An} {Introduction}},
	language = {en},
	journal = {An Introduction},
	author = {Becker, Ingolf},
	pages = {49},
}

@article{brass_adaptive_2020,
	title = {Adaptive governance for the {Internet} of {Things}: {Coping} with emerging security risks},
	abstract = {The Internet of Things (IoT) is a disruptive innovation known for its socio-economic potential, but also for generating unprecedented vulnerabilities and threats. As a dynamic sociotechnical system, the IoT comprises well-known cybersecurity risks and endemic uncertainties that arise as IoT adoption increases and the system evolves. We highlight the impact of these challenges by analyzing how insecure IoT devices pose threats to both consumer protection and the Internet’s infrastructure. While recent regulatory responses are starting to target IoT security risks, crucial deﬁciencies – especially related to the feedback necessary to keep pace with emerging risks and uncertainties – must be addressed. We propose a model of adaptive regulatory governance that integrates the beneﬁts of centralized risk regulatory frameworks with the operational knowledge and mitigation mechanisms developed by epistemic communities that manage day-to-day Internet security. Rather than focusing on the choice of regulatory instruments, this model builds on the “planned adaptive regulation” literature to highlight the need to systematically plan for a knowledge-sharing interface in regulatory governance design for disruptive technologies, facilitating the feedback necessary to address evolving IoT security risks.},
	language = {en},
	author = {Brass, Irina},
	year = {2020},
	pages = {19},
}

@article{anderson_economics_2006,
	title = {The {Economics} of {Information} {Security}},
	language = {en},
	author = {Anderson, Ross and Moore, Tyler},
	month = oct,
	year = {2006},
	pages = {4},
}

@misc{noauthor_talks_nodate,
	title = {Talks: {Securing} a {World} of {Physically} {Capable} {Computers} - {Schneier} on {Security}},
	url = {https://www.schneier.com/talks/archives/2020/12/securing-a-world-of-physically-capable-computers.html},
	urldate = {2021-01-18},
}

@article{kleinberg_week_nodate,
	title = {{WEEK} 2: {WEB} {DATA} {COLLECTION} 2 {SECU0057}},
	language = {en},
	author = {Kleinberg, Bennett},
	pages = {62},
}

@article{kleinberg_week_nodate-1,
	title = {{WEEK} 1: {WEB} {DATA} {COLLECTION} 1 {SECU0057}},
	language = {en},
	author = {Kleinberg, Bennett},
	pages = {84},
}

@article{bagdasaryan_how_nodate,
	title = {How {To} {Backdoor} {Federated} {Learning}},
	abstract = {Federated models are created by aggregating model updates submitted by participants. To protect conﬁdentiality of the training data, the aggregator by design has no visibility into how these updates are generated. We show that this makes federated learning vulnerable to a model-poisoning attack that is signiﬁcantly more powerful than poisoning attacks that target only the training data. A single or multiple malicious participants can use model replacement to introduce backdoor functionality into the joint model, e.g., modify an image classiﬁer so that it assigns an attacker-chosen label to images with certain features, or force a word predictor to complete certain sentences with an attackerchosen word. We evaluate model replacement under di↵erent assumptions for the standard federated-learning tasks and show that it greatly outperforms training-data poisoning. Federated learning employs secure aggregation to protect conﬁdentiality of participants’ local models and thus cannot detect anomalies in participants’ contributions to the joint model. To demonstrate that anomaly detection would not have been e↵ective in any case, we also develop and evaluate a generic constrain-and-scale technique that incorporates the evasion of defenses into the attacker’s loss function during training.},
	language = {en},
	author = {Bagdasaryan, Eugene and Veit, Andreas and Hua, Yiqing and Estrin, Deborah and Shmatikov, Vitaly},
	pages = {10},
}

@article{yu_salvaging_2020,
	title = {Salvaging {Federated} {Learning} by {Local} {Adaptation}},
	url = {http://arxiv.org/abs/2002.04758},
	abstract = {Federated learning (FL) is a heavily promoted approach for training ML models on sensitive data, e.g., text typed by users on their smartphones. FL is expressly designed for training on data that are unbalanced and non-iid across the participants. To ensure privacy and integrity of the federated model, latest FL approaches use differential privacy or robust aggregation to limit the inﬂuence of “outlier” participants. First, we show that on standard tasks such as nextword prediction, many participants gain no beneﬁt from FL because the federated model is less accurate on their data than the models they can train locally on their own. Second, we show that differential privacy and robust aggregation make this problem worse by further destroying the accuracy of the federated model for many participants.},
	language = {en},
	urldate = {2021-01-13},
	journal = {arXiv:2002.04758 [cs, stat]},
	author = {Yu, Tao and Bagdasaryan, Eugene and Shmatikov, Vitaly},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.04758},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{oecd_regulatory_nodate,
	title = {Regulatory effectiveness in the era of digitalisation},
	url = {https://www.oecd.org/gov/regulatory-policy/Regulatory-effectiveness-in-the-era-of-digitalisation.pdf},
	urldate = {2021-01-11},
	author = {{OECD}},
}

@article{daviter_coping_2017,
	title = {Coping, taming or solving: alternative approaches to the governance of wicked problems},
	volume = {38},
	issn = {0144-2872, 1470-1006},
	shorttitle = {Coping, taming or solving},
	url = {https://www.tandfonline.com/doi/full/10.1080/01442872.2017.1384543},
	doi = {10.1080/01442872.2017.1384543},
	abstract = {One of the truisms of policy analysis is that policy problems are rarely solved. As an ever-increasing number of policy issues are identified as an inherently ill-structured and intractable type of wicked problem, the question of what policy analysis sets out to accomplish has emerged as more central than ever. If solving wicked problems is beyond reach, research on wicked problems needs to provide a clearer understanding of the alternatives. The article identifies and explicates three distinguishable strategies of problem governance: coping, taming and solving. It shows that their intellectual premises and practical implications clearly contrast in core respects. The article argues that none of the identified strategies of problem governance is invariably more suitable for dealing with wicked problems. Rather than advocate for some universally applicable approach to the governance of wicked problems, the article asks under what conditions different ways of governing wicked problems are analytically reasonable and normatively justified. It concludes that a more systematic assessment of alternative approaches of problem governance requires a reorientation of the debate away from the conception of wicked problems as a singular type toward the more focused analysis of different dimensions of problem wickedness.},
	language = {en},
	number = {6},
	urldate = {2021-01-11},
	journal = {Policy Studies},
	author = {Daviter, Falk},
	month = nov,
	year = {2017},
	pages = {571--588},
}

@inproceedings{barthe_differentially_2016,
	address = {Vienna Austria},
	title = {Differentially {Private} {Bayesian} {Programming}},
	isbn = {978-1-4503-4139-4},
	url = {https://dl.acm.org/doi/10.1145/2976749.2978371},
	doi = {10.1145/2976749.2978371},
	abstract = {We present PrivInfer, an expressive framework for writing and verifying diﬀerentially private Bayesian machine learning algorithms. Programs in PrivInfer are written in a rich functional probabilistic programming language with constructs for performing Bayesian inference. Then, diﬀerential privacy of programs is established using a relational reﬁnement type system, in which reﬁnements on probability types are indexed by a metric on distributions. Our framework leverages recent developments in Bayesian inference, probabilistic programming languages, and in relational reﬁnement types. We demonstrate the expressiveness of PrivInfer by verifying privacy for several examples of private Bayesian inference.},
	language = {en},
	urldate = {2021-01-10},
	booktitle = {Proceedings of the 2016 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Barthe, Gilles and Farina, Gian Pietro and Gaboardi, Marco and Arias, Emilio Jesus Gallego and Gordon, Andy and Hsu, Justin and Strub, Pierre-Yves},
	month = oct,
	year = {2016},
	pages = {68--79},
}

@article{roy_airavat_2010,
	title = {Airavat: {Security} and {Privacy} for {MapReduce}},
	abstract = {We present Airavat, a MapReduce-based system which provides strong security and privacy guarantees for distributed computations on sensitive data. Airavat is a novel integration of mandatory access control and differential privacy. Data providers control the security policy for their sensitive data, including a mathematical bound on potential privacy violations. Users without security expertise can perform computations on the data, but Airavat conﬁnes these computations, preventing information leakage beyond the data provider’s policy.},
	language = {en},
	journal = {NSDI'10: Proceedings of the 7th USENIX conference on Networked systems design and implementation},
	author = {Roy, Indrajit and Setty, Srinath T V and Kilzer, Ann and Shmatikov, Vitaly and Witchel, Emmett},
	year = {2010},
	pages = {16},
}

@article{gaboardi_psi_2018,
	title = {{PSI}: a {Private} data {Sharing} {Interface}},
	shorttitle = {{PSI} (\{{\textbackslash}{Psi}\})},
	url = {http://arxiv.org/abs/1609.04340},
	abstract = {We provide an overview of the design of PSI (“a Private data Sharing Interface”), a system we are developing to enable researchers in the social sciences and other ﬁelds to share and explore privacy-sensitive datasets with the strong privacy protections of diﬀerential privacy.},
	language = {en},
	urldate = {2021-01-10},
	journal = {arXiv:1609.04340 [cs, stat]},
	author = {Gaboardi, Marco and Honaker, James and King, Gary and Murtagh, Jack and Nissim, Kobbi and Ullman, Jonathan and Vadhan, Salil},
	month = aug,
	year = {2018},
	note = {arXiv: 1609.04340},
	keywords = {Computer Science - Computers and Society, Computer Science - Cryptography and Security, Statistics - Methodology},
}

@article{murtagh_usable_2018,
	title = {Usable {Differential} {Privacy}: {A} {Case} {Study} with {PSI}},
	shorttitle = {Usable {Differential} {Privacy}},
	url = {http://arxiv.org/abs/1809.04103},
	abstract = {Diﬀerential privacy is a promising framework for addressing the privacy concerns in sharing sensitive datasets for others to analyze. However diﬀerential privacy is a highly technical area and current deployments often require experts to write code, tune parameters, and optimize the trade-oﬀ between the privacy and accuracy of statistical releases. For diﬀerential privacy to achieve its potential for wide impact, it is important to design usable systems that enable diﬀerential privacy to be used by ordinary data owners and analysts. PSI is a tool that was designed for this purpose, allowing researchers to release useful diﬀerentially private statistical information about their datasets without being experts in computer science, statistics, or privacy. We conducted a thorough usability study of PSI to test whether it accomplishes its goal of usability by non-experts. The usability test illuminated which features of PSI are most user-friendly and prompted us to improve aspects of the tool that caused confusion. The test also highlighted some general principles and lessons for designing usable systems for diﬀerential privacy, which we discuss in depth.},
	language = {en},
	urldate = {2021-01-10},
	journal = {arXiv:1809.04103 [cs]},
	author = {Murtagh, Jack and Taylor, Kathryn and Kellaris, George and Vadhan, Salil},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.04103},
	keywords = {Computer Science - Human-Computer Interaction},
}

@inproceedings{eigner_differential_2013,
	address = {New Orleans, LA},
	title = {Differential {Privacy} by {Typing} in {Security} {Protocols}},
	isbn = {978-0-7695-5031-2},
	url = {https://ieeexplore.ieee.org/document/6595834/},
	doi = {10.1109/CSF.2013.25},
	abstract = {Differential privacy is a conﬁdentiality property for database queries which allows for the release of statistical information about the content of a database without disclosing personal data. The variety of database queries and enforcement mechanisms has recently sparked the development of a number of mechanized proof techniques for differential privacy.},
	language = {en},
	urldate = {2021-01-10},
	booktitle = {2013 {IEEE} 26th {Computer} {Security} {Foundations} {Symposium}},
	publisher = {IEEE},
	author = {Eigner, Fabienne and Maffei, Matteo},
	month = jun,
	year = {2013},
	pages = {272--286},
}

@inproceedings{dantoni_sensitivity_2013,
	address = {Boston, Massachusetts, USA},
	title = {Sensitivity analysis using type-based constraints},
	isbn = {978-1-4503-2380-2},
	url = {http://dl.acm.org/citation.cfm?doid=2505351.2505353},
	doi = {10.1145/2505351.2505353},
	abstract = {Function sensitivity—how much the result of a function can change with respect to linear changes in the input—is a key concept in many research areas. For instance, in differential privacy, one of the most common mechanisms for turning a (possibly privacy-leaking) query into a differentially private one involves establishing a bound on its sensitivity.},
	language = {en},
	urldate = {2021-01-10},
	booktitle = {Proceedings of the 1st annual workshop on {Functional} programming concepts in domain-specific languages - {FPCDSL} '13},
	publisher = {ACM Press},
	author = {D'Antoni, Loris and Gaboardi, Marco and Gallego Arias, Emilio Jesús and Haeberlen, Andreas and Pierce, Benjamin},
	year = {2013},
	pages = {43},
}

@inproceedings{de_amorim_really_2014,
	address = {Boston, MA, USA},
	title = {Really {Natural} {Linear} {Indexed} {Type} {Checking}},
	isbn = {978-1-4503-3284-2},
	url = {http://dl.acm.org/citation.cfm?doid=2746325.2746335},
	doi = {10.1145/2746325.2746335},
	abstract = {Recent works have shown the power of linear indexed type systems for enforcing complex program properties. These systems combine linear types with a language of type-level indices, allowing more ﬁne-grained analyses. Such systems have been fruitfully applied in diverse domains, including implicit complexity and differential privacy.},
	language = {en},
	urldate = {2021-01-10},
	booktitle = {Proceedings of the 26nd 2014 {International} {Symposium} on {Implementation} and {Application} of {Functional} {Languages} - {IFL} '14},
	publisher = {ACM Press},
	author = {de Amorim, Arthur Azevedo and Gaboardi, Marco and Gallego Arias, Emilio Jesús and Hsu, Justin},
	year = {2014},
	pages = {1--12},
}

@article{gaboardi_linear_2013,
	title = {Linear {Dependent} {Types} for {Differential} {Privacy}},
	abstract = {Differential privacy offers a way to answer queries about sensitive information while providing strong, provable privacy guarantees, ensuring that the presence or absence of a single individual in the database has a negligible statistical effect on the query’s result. Proving that a given query has this property involves establishing a bound on the query’s sensitivity—how much its result can change when a single record is added or removed.},
	language = {en},
	journal = {POPL’13},
	author = {Gaboardi, Marco and Haeberlen, Andreas and Hsu, Justin and Narayan, Arjun and Pierce, Benjamin C},
	year = {2013},
	pages = {14},
}

@article{zhang_fuzzi_2019,
	title = {Fuzzi: a three-level logic for differential privacy},
	volume = {3},
	issn = {2475-1421, 2475-1421},
	shorttitle = {Fuzzi},
	url = {https://dl.acm.org/doi/10.1145/3341697},
	doi = {10.1145/3341697},
	abstract = {HENGCHU ZHANG, University of Pennsylvania, USA EDO ROTH, University of Pennsylvania, USA ANDREAS HAEBERLEN, University of Pennsylvania, USA BENJAMIN C. PIERCE, University of Pennsylvania, USA AARON ROTH, University of Pennsylvania, USA Curators of sensitive datasets sometimes need to know whether queries against the data are differentially private.Two sorts of logics have been proposed for checking this property: (1) type systems and other static analyses, which fully automate straightforward reasoning with concepts like łprogram sensitivityž and łprivacy loss,ž and (2) full-blown program logics such as apRHL (an approximate, probabilistic, relational Hoare logic),which support more flexible reasoning about subtle privacy-preserving algorithmic techniques but offer only minimal automation. We propose a three-level logic for differential privacy in an imperative setting and present a prototype implementation called Fuzzi. Fuzzi’s lowest level is a general-purpose logic; its middle level is apRHL; and its top level is a novel sensitivity logic adapted from the linear-logic-inspired type system of Fuzz, a differentially private functional language.The key novelty is a high degree of integration between the sensitivity logic and the two lower-level logics: the judgments and proofs of the sensitivity logic can be easily translated into apRHL; conversely, privacy properties of key algorithmic building blocks can be proved manually in apRHL and the base logic, then packaged up as typing rules that can be applied by a checker for the sensitivity logic to automatically construct privacy proofs for composite programs of arbitrary size. We demonstrate Fuzzi’s utility by implementing four different private machine-learning algorithms and showing that Fuzzi’s checker is able to derive tight sensitivity bounds. CCS Concepts: • Theory of computation → Programming logic; Program specifications.},
	language = {en},
	number = {ICFP},
	urldate = {2021-01-10},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Zhang, Hengchu and Roth, Edo and Haeberlen, Andreas and Pierce, Benjamin C. and Roth, Aaron},
	month = jul,
	year = {2019},
	pages = {1--28},
}

@incollection{goos_taste_1993,
	address = {Berlin, Heidelberg},
	title = {A taste of linear logic},
	volume = {711},
	isbn = {978-3-540-57182-7 978-3-540-47927-7},
	url = {http://link.springer.com/10.1007/3-540-57182-5_12},
	abstract = {This tutorial paper provides an introduction to intuitionistic logic and linear logic, and shows how they correspond to type systems for functional languages via the notion of ‘Propositions as Types’. The presentation of linear logic is simpliﬁed by basing it on the Logic of Unity. An application to the array update problem is brieﬂy discussed.},
	language = {en},
	urldate = {2021-01-10},
	booktitle = {Mathematical {Foundations} of {Computer} {Science} 1993},
	publisher = {Springer Berlin Heidelberg},
	author = {Wadler, Philip},
	editor = {Goos, G. and Hartmanis, J. and Borzyszkowski, Andrzej M. and Sokołowski, Stefan},
	year = {1993},
	doi = {10.1007/3-540-57182-5_12},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {185--210},
}

@article{winograd-cort_framework_2017,
	title = {A framework for adaptive differential privacy},
	volume = {1},
	issn = {2475-1421, 2475-1421},
	url = {https://dl.acm.org/doi/10.1145/3110254},
	doi = {10.1145/3110254},
	language = {en},
	number = {ICFP},
	urldate = {2021-01-09},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Winograd-Cort, Daniel and Haeberlen, Andreas and Roth, Aaron and Pierce, Benjamin C.},
	month = aug,
	year = {2017},
	pages = {1--29},
}

@inproceedings{andrysco_subnormal_2015,
	address = {San Jose, CA},
	title = {On {Subnormal} {Floating} {Point} and {Abnormal} {Timing}},
	isbn = {978-1-4673-6949-7},
	url = {https://ieeexplore.ieee.org/document/7163051/},
	doi = {10.1109/SP.2015.44},
	abstract = {We identify a timing channel in the ﬂoating point instructions of modern x86 processors: the running time of ﬂoating point addition and multiplication instructions can vary by two orders of magnitude depending on their operands. We develop a benchmark measuring the timing variability of ﬂoating point operations and report on its results. We use ﬂoating point data timing variability to demonstrate practical attacks on the security of the Firefox browser (versions 23 through 27) and the Fuzz differentially private database. Finally, we initiate the study of mitigations to ﬂoating point data timing channels with libfixedtimefixedpoint, a new ﬁxed-point, constant-time math library.},
	language = {en},
	urldate = {2021-01-08},
	booktitle = {2015 {IEEE} {Symposium} on {Security} and {Privacy}},
	publisher = {IEEE},
	author = {Andrysco, Marc and Kohlbrenner, David and Mowery, Keaton and Jhala, Ranjit and Lerner, Sorin and Shacham, Hovav},
	month = may,
	year = {2015},
	pages = {623--639},
}

@article{balcer_differential_2019,
	title = {Differential {Privacy} on {Finite} {Computers}},
	url = {http://arxiv.org/abs/1709.05396},
	abstract = {We consider the problem of designing and analyzing diﬀerentially private algorithms that can be implemented on discrete models of computation in strict polynomial time, motivated by known attacks on ﬂoating point implementations of real-arithmetic diﬀerentially private algorithms (Mironov, CCS 2012) and the potential for timing attacks on expected polynomialtime algorithms. As a case study, we examine the basic problem of approximating the histogram of a categorical dataset over a possibly large data universe X . The classic Laplace Mechanism (Dwork, McSherry, Nissim, Smith, TCC 2006 and J. Privacy \& Conﬁdentiality 2017) does not satisfy our requirements, as it is based on real arithmetic, and natural discrete analogues, such as the Geometric Mechanism (Ghosh, Roughgarden, Sundarajan, STOC 2009 and SICOMP 2012), take time at least linear in {\textbar}X {\textbar}, which can be exponential in the bit length of the input. In this paper, we provide strict polynomial-time discrete algorithms for approximate histograms whose simultaneous accuracy (the maximum error over all bins) matches that of the Laplace Mechanism up to constant factors, while retaining the same (pure) diﬀerential privacy guarantee. One of our algorithms produces a sparse histogram as output. Its “per-bin accuracy” (the error on individual bins) is worse than that of the Laplace Mechanism by a factor of log {\textbar}X {\textbar}, but we prove a lower bound showing that this is necessary for any algorithm that produces a sparse histogram. A second algorithm avoids this lower bound, and matches the per-bin accuracy of the Laplace Mechanism, by producing a compact and eﬃciently computable representation of a dense histogram; it is based on an (n + 1)-wise independent implementation of an appropriately clamped version of the Discrete Geometric Mechanism.},
	language = {en},
	urldate = {2021-01-08},
	journal = {arXiv:1709.05396 [cs]},
	author = {Balcer, Victor and Vadhan, Salil},
	month = jan,
	year = {2019},
	note = {arXiv: 1709.05396},
	keywords = {Computer Science - Data Structures and Algorithms},
}

@article{albarghouthi_synthesizing_2018,
	title = {Synthesizing coupling proofs of differential privacy},
	volume = {2},
	issn = {2475-1421, 2475-1421},
	url = {https://dl.acm.org/doi/10.1145/3158146},
	doi = {10.1145/3158146},
	language = {en},
	number = {POPL},
	urldate = {2021-01-02},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Albarghouthi, Aws and Hsu, Justin},
	month = jan,
	year = {2018},
	pages = {1--30},
}

@inproceedings{bichsel_dp-finder_2018,
	address = {Toronto Canada},
	title = {{DP}-{Finder}: {Finding} {Differential} {Privacy} {Violations} by {Sampling} and {Optimization}},
	isbn = {978-1-4503-5693-0},
	shorttitle = {{DP}-{Finder}},
	url = {https://dl.acm.org/doi/10.1145/3243734.3243863},
	doi = {10.1145/3243734.3243863},
	abstract = {We present DP-Finder, a novel approach and system that automatically derives lower bounds on the differential privacy enforced by algorithms. Lower bounds are practically useful as they can show tightness of existing upper bounds or even identify incorrect upper bounds. Computing a lower bound involves searching for a counterexample, defined by two neighboring inputs and a set of outputs, that identifies a large privacy violation. This is an inherently hard problem as finding such a counterexample involves inspecting a large (usually infinite) and sparse search space.},
	language = {en},
	urldate = {2021-01-06},
	booktitle = {Proceedings of the 2018 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Bichsel, Benjamin and Gehr, Timon and Drachsler-Cohen, Dana and Tsankov, Petar and Vechev, Martin},
	month = oct,
	year = {2018},
	pages = {508--524},
}

@article{mcsherry_privacy_2010,
	title = {Privacy integrated queries: an extensible platform for privacy-preserving data analysis},
	volume = {53},
	url = {https://dl.acm.org/doi/pdf/10.1145/1810891.1810916},
	doi = {10.1145/1810891.1810916},
	abstract = {Privacy Integrated Queries (PINQ) is an extensible data analysis platform designed to provide unconditional privacy guarantees for the records of the underlying data sets. PINQ provides analysts with access to records through an SQLlike declarative language (LINQ) amidst otherwise arbitrary C\# code. At the same time, the design of PINQ’s analysis language and its careful implementation provide formal guarantees of differential privacy for any and all uses of the platform. PINQ’s guarantees require no trust placed in the expertise or diligence of the analysts, broadening the scope for design and deployment of privacy-preserving data analyses, especially by privacy nonexperts.},
	language = {en},
	journal = {Communications of the ACM},
	author = {McSherry, Frank},
	month = sep,
	year = {2010},
	pages = {9},
}

@inproceedings{hay_principled_2016,
	address = {San Francisco California USA},
	title = {Principled {Evaluation} of {Differentially} {Private} {Algorithms} using {DPBench}},
	isbn = {978-1-4503-3531-7},
	url = {https://dl.acm.org/doi/10.1145/2882903.2882931},
	doi = {10.1145/2882903.2882931},
	abstract = {Differential privacy has become the dominant standard in the re­ search community for strong privacy protection. There has been a ﬂood of research into query answering algorithms that meet this standard. Algorithms are becoming increasingly complex, and in particular, the performance of many emerging algorithms is data dependent, meaning the distribution of the noise added to query an­ swers may change depending on the input data. Theoretical analy­ sis typically only considers the worst case, making empirical study of average case performance increasingly important.},
	language = {en},
	urldate = {2021-01-05},
	booktitle = {Proceedings of the 2016 {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Hay, Michael and Machanavajjhala, Ashwin and Miklau, Gerome and Chen, Yan and Zhang, Dan},
	month = jun,
	year = {2016},
	pages = {139--154},
}

@article{zhang_privacy_2020,
	title = {Privacy for {All}: {Demystify} {Vulnerability} {Disparity} of {Differential} {Privacy} against {Membership} {Inference} {Attack}},
	shorttitle = {Privacy for {All}},
	url = {http://arxiv.org/abs/2001.08855},
	abstract = {Machine learning algorithms, when applied to sensitive data, pose potential threat to privacy. A growing body of prior work has demonstrated that membership inference attack (MIA) can disclose speciﬁc private information in the training data to an attacker. Meanwhile, the algorithmic fairness of machine learning has increasingly caught attention from both academia and industry. Algorithmic fairness ensures that the machine learning models do not discriminate a particular demographic group of individuals (e.g., black and female people). Given that MIA is indeed a learning model, it raises a serious concern if MIA “fairly” treats all groups of individuals equally. In other words, whether a particular group is more vulnerable against MIA than the other groups. This paper examines the algorithmic fairness issue in the context of MIA and its defenses. First, for fairness evaluation, it formalizes the notation of vulnerability disparity (VD) to quantify the diﬀerence of MIA treatment on diﬀerent demographic groups. Second, it evaluates VD on four real-world datasets, and shows that VD indeed exists in these datasets. Third, it examines the impacts of diﬀerential privacy, as a defense mechanism of MIA, on VD. The results show that although DP brings signiﬁcant change on VD, it cannot eliminate VD completely. Therefore, fourth, it designs a new mitigation algorithm named F P to reduce VD. An extensive set of experimental results demonstrate that F P can eﬀectively reduce VD for both with and without the DP deployment.},
	language = {en},
	urldate = {2021-01-04},
	journal = {arXiv:2001.08855 [cs]},
	author = {Zhang, Bo and Yu, Ruotong and Sun, Haipei and Li, Yanying and Xu, Jun and Wang, Hui},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.08855},
	keywords = {Computer Science - Cryptography and Security},
}

@article{pyrgelis_knock_2017,
	title = {Knock {Knock}, {Who}'s {There}? {Membership} {Inference} on {Aggregate} {Location} {Data}},
	shorttitle = {Knock {Knock}, {Who}'s {There}?},
	url = {http://arxiv.org/abs/1708.06145},
	abstract = {Aggregate location data is often used to support smart services and applications, e.g., generating live trafﬁc maps or predicting visits to businesses. In this paper, we present the ﬁrst study on the feasibility of membership inference attacks on aggregate location time-series. We introduce a game-based deﬁnition of the adversarial task, and cast it as a classiﬁcation problem where machine learning can be used to distinguish whether or not a target user is part of the aggregates.},
	language = {en},
	urldate = {2021-01-04},
	journal = {arXiv:1708.06145 [cs]},
	author = {Pyrgelis, Apostolos and Troncoso, Carmela and De Cristofaro, Emiliano},
	month = nov,
	year = {2017},
	note = {arXiv: 1708.06145},
	keywords = {Computer Science - Cryptography and Security},
}

@article{truex_towards_2019,
	title = {Towards {Demystifying} {Membership} {Inference} {Attacks}},
	url = {http://arxiv.org/abs/1807.09173},
	abstract = {Membership inference attacks seek to infer membership of individual training instances of a model to which an adversary has black-box access through a machine learning-as-a-service API. In providing an in-depth characterization of membership privacy risks against machine learning models, this paper presents a comprehensive study towards demystifying membership inference attacks from two complimentary perspectives. First, we provide a generalized formulation of the development of a black-box membership inference attack model. Second, we characterize the importance of model choice on model vulnerability through a systematic evaluation of a variety of machine learning models and model combinations using multiple datasets. Through formal analysis and empirical evidence from extensive experimentation, we characterize under what conditions a model may be vulnerable to such black-box membership inference attacks. We show that membership inference vulnerability is data-driven and corresponding attack models are largely transferable. Though different model types display different vulnerabilities to membership inference, so do different datasets. Our empirical results additionally show that (1) using the type of target model under attack within the attack model may not increase attack effectiveness and (2) collaborative learning exposes vulnerabilities to membership inference risks when the adversary is a participant. We also discuss countermeasure and mitigation strategies.},
	language = {en},
	urldate = {2021-01-04},
	journal = {arXiv:1807.09173 [cs]},
	author = {Truex, Stacey and Liu, Ling and Gursoy, Mehmet Emre and Yu, Lei and Wei, Wenqi},
	month = feb,
	year = {2019},
	note = {arXiv: 1807.09173},
	keywords = {Computer Science - Cryptography and Security},
}

@article{choo_label-only_2020,
	title = {Label-{Only} {Membership} {Inference} {Attacks}},
	url = {http://arxiv.org/abs/2007.14321},
	abstract = {Membership inference attacks are one of the simplest forms of privacy leakage for machine learning models: given a data point and model, determine whether the point was used to train the model. Existing membership inference attacks exploit models’ abnormal conﬁdence when queried on their training data. These attacks do not apply if the adversary only gets access to models’ predicted labels, without a conﬁdence measure.},
	language = {en},
	urldate = {2021-01-04},
	journal = {arXiv:2007.14321 [cs, stat]},
	author = {Choo, Christopher A. Choquette and Tramer, Florian and Carlini, Nicholas and Papernot, Nicolas},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.14321},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{truex_effects_2019,
	title = {Effects of {Differential} {Privacy} and {Data} {Skewness} on {Membership} {Inference} {Vulnerability}},
	url = {http://arxiv.org/abs/1911.09777},
	abstract = {Membership inference attacks seek to infer the membership of individual training instances of a privately trained model. This paper presents a membership privacy analysis and evaluation system, called MPLens, with three unique contributions. First, through MPLens, we demonstrate how membership inference attack methods can be leveraged in adversarial machine learning. Second, through MPLens, we highlight how the vulnerability of pre-trained models under membership inference attack is not uniform across all classes, particularly when the training data itself is skewed. We show that risk from membership inference attacks is routinely increased when models use skewed training data. Finally, we investigate the effectiveness of differential privacy as a mitigation technique against membership inference attacks. We discuss the trade-offs of implementing such a mitigation strategy with respect to the model complexity, the learning task complexity, the dataset complexity and the privacy parameter settings. Our empirical results reveal that (1) minority groups within skewed datasets display increased risk for membership inference and (2) differential privacy presents many challenging trade-offs as a mitigation technique to membership inference risk.},
	language = {en},
	urldate = {2021-01-04},
	journal = {arXiv:1911.09777 [cs, stat]},
	author = {Truex, Stacey and Liu, Ling and Gursoy, Mehmet Emre and Wei, Wenqi and Yu, Lei},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.09777},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{rahman_membership_2018,
	title = {Membership {Inference} {Attack} against {Differentially} {Private} {Deep} {Learning} {Model}},
	abstract = {The unprecedented success of deep learning is largely dependent on the availability of massive amount of training data. In many cases, these data are crowd-sourced and may contain sensitive and conﬁdential information, therefore, pose privacy concerns. As a result, privacy-preserving deep learning has been gaining increasing focus nowadays. One of the promising approaches for privacy-preserving deep learning is to employ differential privacy during model training which aims to prevent the leakage of sensitive information about the training data via the trained model. While these models are considered to be immune to privacy attacks, with the advent of recent and sophisticated attack models, it is not clear how well these models trade-off utility for privacy. In this paper, we systematically study the impact of a sophisticated machine learning based privacy attack called the membership inference attack against a state-of-the-art differentially private deep model. More speciﬁcally, given a differentially private deep model with its associated utility, we investigate how much we can infer about the model’s training data. Our experimental results show that differentially private deep models may keep their promise to provide privacy protection against strong adversaries by only offering poor model utility, while exhibit moderate vulnerability to the membership inference attack when they offer an acceptable utility. For evaluating our experiments, we use the CIFAR-10 and MNIST datasets and the corresponding classiﬁcation tasks.},
	language = {en},
	author = {Rahman, Atiqur and Rahman, Tanzila and Laganiere, Robert and Mohammed, Noman and Wang, Yang},
	year = {2018},
	pages = {19},
}

@article{dwork_difficulties_2010,
	title = {On the {Difficulties} of {Disclosure} {Prevention} in {Statistical} {Databases} or {The} {Case} for {Differential} {Privacy}},
	volume = {2},
	issn = {2575-8527},
	url = {https://journalprivacyconfidentiality.org/index.php/jpc/article/view/585},
	doi = {10.29012/jpc.v2i1.585},
	abstract = {In 1977 Tore Dalenius articulated a desideratum for statistical databases: nothing about

an individual should be learnable from the database that cannot be learned without access to the

database. We give a general impossibility result showing that a natural formalization of Dalenius’

goal cannot be achieved if the database is useful. The key obstacle is the side information that

may be available to an adversary. Our results hold under very general conditions regarding the

database, the notion of privacy violation, and the notion of utility.

Contrary to intuition, a variant of the result threatens the privacy even of someone not in

the database. This state of affairs motivated the notion of differential privacy [15, 16], a strong

ad omnia privacy which, intuitively, captures the increased risk to one’s privacy incurred by

participating in a database.},
	language = {en},
	number = {1},
	urldate = {2021-01-04},
	journal = {Journal of Privacy and Confidentiality},
	author = {Dwork, Cynthia and Naor, Moni},
	month = sep,
	year = {2010},
}

@inproceedings{calandrino_you_2011,
	address = {Oakland, CA, USA},
	title = {"{You} {Might} {Also} {Like}:" {Privacy} {Risks} of {Collaborative} {Filtering}},
	isbn = {978-1-4577-0147-4},
	shorttitle = {"{You} {Might} {Also} {Like}},
	url = {http://ieeexplore.ieee.org/document/5958032/},
	doi = {10.1109/SP.2011.40},
	abstract = {Many commercial websites use recommender systems to help customers locate products and content. Modern recommenders are based on collaborative ﬁltering: they use patterns learned from users’ behavior to make recommendations, usually in the form of related-items lists. The scale and complexity of these systems, along with the fact that their outputs reveal only relationships between items (as opposed to information about users), may suggest that they pose no meaningful privacy risk.},
	language = {en},
	urldate = {2021-01-04},
	booktitle = {2011 {IEEE} {Symposium} on {Security} and {Privacy}},
	publisher = {IEEE},
	author = {Calandrino, Joseph A. and Kilzer, Ann and Narayanan, Arvind and Felten, Edward W. and Shmatikov, Vitaly},
	month = may,
	year = {2011},
	pages = {231--246},
}

@article{papernot_semi-supervised_2017,
	title = {Semi-supervised {Knowledge} {Transfer} for {Deep} {Learning} from {Private} {Training} {Data}},
	url = {http://arxiv.org/abs/1610.05755},
	abstract = {Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information.},
	language = {en},
	urldate = {2021-01-04},
	journal = {arXiv:1610.05755 [cs, stat]},
	author = {Papernot, Nicolas and Abadi, Martín and Erlingsson, Úlfar and Goodfellow, Ian and Talwar, Kunal},
	month = mar,
	year = {2017},
	note = {arXiv: 1610.05755},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{shokri_privacy-preserving_2015,
	address = {Denver Colorado USA},
	title = {Privacy-{Preserving} {Deep} {Learning}},
	isbn = {978-1-4503-3832-5},
	url = {https://dl.acm.org/doi/10.1145/2810103.2813687},
	doi = {10.1145/2810103.2813687},
	abstract = {Deep learning based on artiﬁcial neural networks is a very popular approach to modeling, classifying, and recognizing complex data such as images, speech, and text. The unprecedented accuracy of deep learning methods has turned them into the foundation of new AI-based services on the Internet. Commercial companies that collect user data on a large scale have been the main beneﬁciaries of this trend since the success of deep learning techniques is directly proportional to the amount of data available for training.},
	language = {en},
	urldate = {2021-01-04},
	booktitle = {Proceedings of the 22nd {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Shokri, Reza and Shmatikov, Vitaly},
	month = oct,
	year = {2015},
	pages = {1310--1321},
}

@misc{frank_mcsherry_ubers_nodate,
	title = {Uber's differential privacy .. probably isn't},
	url = {https://github.com/frankmcsherry/blog},
	abstract = {Some notes on things I find interesting and important. - frankmcsherry/blog},
	language = {en},
	urldate = {2021-01-02},
	journal = {GitHub},
	author = {{Frank McSherry}},
}

@incollection{vaudenay_our_2006,
	address = {Berlin, Heidelberg},
	title = {Our {Data}, {Ourselves}: {Privacy} {Via} {Distributed} {Noise} {Generation}},
	volume = {4004},
	isbn = {978-3-540-34546-6 978-3-540-34547-3},
	shorttitle = {Our {Data}, {Ourselves}},
	url = {http://link.springer.com/10.1007/11761679_29},
	language = {en},
	urldate = {2021-01-04},
	booktitle = {Advances in {Cryptology} - {EUROCRYPT} 2006},
	publisher = {Springer Berlin Heidelberg},
	author = {Dwork, Cynthia and Kenthapadi, Krishnaram and McSherry, Frank and Mironov, Ilya and Naor, Moni},
	editor = {Vaudenay, Serge},
	year = {2006},
	doi = {10.1007/11761679_29},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {486--503},
}

@article{gazeau_preserving_2016,
	title = {Preserving differential privacy under finite-precision semantics},
	volume = {655},
	issn = {03043975},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304397516000268},
	doi = {10.1016/j.tcs.2016.01.015},
	language = {en},
	urldate = {2021-01-04},
	journal = {Theoretical Computer Science},
	author = {Gazeau, Ivan and Miller, Dale and Palamidessi, Catuscia},
	month = dec,
	year = {2016},
	pages = {92--108},
}

@article{desfontaines_sok_2020,
	title = {{SoK}: {Differential} {Privacies}},
	shorttitle = {{SoK}},
	url = {http://arxiv.org/abs/1906.01337},
	abstract = {Shortly after it was ﬁrst introduced in 2006, diﬀerential privacy became the ﬂagship data privacy deﬁnition. Since then, numerous variants and extensions were proposed to adapt it to diﬀerent scenarios and attacker models. In this work, we propose a systematic taxonomy of these variants and extensions. We list all data privacy deﬁnitions based on diﬀerential privacy, and partition them into seven categories, depending on which aspect of the original deﬁnition is modiﬁed.},
	language = {en},
	urldate = {2021-01-03},
	journal = {arXiv:1906.01337 [cs]},
	author = {Desfontaines, Damien and Pejó, Balázs},
	month = jul,
	year = {2020},
	note = {arXiv: 1906.01337},
	keywords = {Computer Science - Cryptography and Security},
}

@article{roth_orchard_nodate,
	title = {Orchard: {Differentially} {Private} {Analytics} at {Scale}},
	abstract = {This paper presents Orchard, a system that can answer queries about sensitive data that is held by millions of user devices, with strong differential privacy guarantees. Orchard combines high accuracy with good scalability, and it uses only a single untrusted party to facilitate the query. Moreover, whereas previous solutions that shared these properties were custombuilt for speciﬁc queries, Orchard is general and can accept a wide range of queries. Orchard accomplishes this by rewriting queries into a distributed protocol that can be executed efﬁciently at scale, using cryptographic primitives. Our prototype of Orchard can execute 14 out of 17 queries chosen from the literature; to our knowledge, no other system can handle more than one of them in this setting. And the costs are moderate: each user device typically needs only a few megabytes of trafﬁc and a few minutes of computation time. Orchard also includes a novel defense against malicious users who attempt to distort the results of a query.},
	language = {en},
	author = {Roth, Edo and Zhang, Hengchu and Haeberlen, Andreas and Pierce, Benjamin C},
	pages = {18},
}

@article{near_duet_2019,
	title = {Duet: an expressive higher-order language and linear type system for statically enforcing differential privacy},
	volume = {3},
	issn = {2475-1421, 2475-1421},
	shorttitle = {Duet},
	url = {https://dl.acm.org/doi/10.1145/3360598},
	doi = {10.1145/3360598},
	language = {en},
	number = {OOPSLA},
	urldate = {2021-01-03},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Near, Joseph P. and Darais, David and Abuah, Chike and Stevens, Tim and Gaddamadugu, Pranav and Wang, Lun and Somani, Neel and Zhang, Mu and Sharma, Nikhil and Shan, Alex and Song, Dawn},
	month = oct,
	year = {2019},
	pages = {1--30},
}

@inproceedings{mohan_gupt_2012,
	address = {Scottsdale, Arizona, USA},
	title = {{GUPT}: privacy preserving data analysis made easy},
	isbn = {978-1-4503-1247-9},
	shorttitle = {{GUPT}},
	url = {http://dl.acm.org/citation.cfm?doid=2213836.2213876},
	doi = {10.1145/2213836.2213876},
	abstract = {It is often highly valuable for organizations to have their data analyzed by external agents. However, any program that computes on potentially sensitive data risks leaking information through its output. Diﬀerential privacy provides a theoretical framework for processing data while protecting the privacy of individual records in a dataset. Unfortunately, it has seen limited adoption because of the loss in output accuracy, the diﬃculty in making programs diﬀerentially private, lack of mechanisms to describe the privacy budget in a programmer’s utilitarian terms, and the challenging requirement that data owners and data analysts manually distribute the limited privacy budget between queries.},
	language = {en},
	urldate = {2021-01-02},
	booktitle = {Proceedings of the 2012 international conference on {Management} of {Data} - {SIGMOD} '12},
	publisher = {ACM Press},
	author = {Mohan, Prashanth and Thakurta, Abhradeep and Shi, Elaine and Song, Dawn and Culler, David},
	year = {2012},
	pages = {349},
}

@article{johnson_towards_2018,
	title = {Towards practical differential privacy for {SQL} queries},
	volume = {11},
	issn = {21508097},
	url = {http://dl.acm.org/citation.cfm?doid=3187009.3177733},
	doi = {10.1145/3187009.3177733},
	abstract = {Differential privacy promises to enable general data analytics while protecting individual privacy, but existing differential privacy mechanisms do not support the wide variety of features and databases used in real-world SQL-based analytics systems. This paper presents the ﬁrst practical approach for differential privacy of SQL queries. Using 8.1 million real-world queries, we conduct an empirical study to determine the requirements for practical differential privacy, and discuss limitations of previous approaches in light of these requirements. To meet these requirements we propose elastic sensitivity, a novel method for approximating the local sensitivity of queries with general equijoins. We prove that elastic sensitivity is an upper bound on local sensitivity and can therefore be used to enforce differential privacy using any local sensitivity-based mechanism.},
	language = {en},
	number = {5},
	urldate = {2021-01-02},
	journal = {Proceedings of the VLDB Endowment},
	author = {Johnson, Noah and Near, Joseph P. and Song, Dawn},
	month = jan,
	year = {2018},
	pages = {526--539},
}

@article{barthe_programming_2016,
	title = {Programming language techniques for differential privacy},
	volume = {3},
	issn = {2372-3491, 2372-3491},
	url = {https://dl.acm.org/doi/10.1145/2893582.2893591},
	doi = {10.1145/2893582.2893591},
	language = {en},
	number = {1},
	urldate = {2020-12-30},
	journal = {ACM SIGLOG News},
	author = {Barthe, Gilles and Gaboardi, Marco and Hsu, Justin and Pierce, Benjamin},
	month = feb,
	year = {2016},
	pages = {34--53},
}

@article{farina_coupled_2020,
	title = {Coupled {Relational} {Symbolic} {Execution} for {Differential} {Privacy}},
	url = {http://arxiv.org/abs/2007.12987},
	abstract = {Diﬀerential privacy is a de facto standard in data privacy with applications in the private and public sectors. Most of the techniques that achieve diﬀerential privacy are based on a judicious use of randomness. However, reasoning about randomized programs is diﬃcult and error prone. For this reason, several techniques have been recently proposed to support designer in proving programs diﬀerentially private or in ﬁnding violations to it.},
	language = {en},
	urldate = {2020-12-30},
	journal = {arXiv:2007.12987 [cs]},
	author = {Farina, Gian Pietro and Chong, Stephen and Gaboardi, Marco},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.12987},
	keywords = {Computer Science - Logic in Computer Science, Computer Science - Programming Languages},
}

@inproceedings{wang_checkdp_2020,
	address = {Virtual Event USA},
	title = {{CheckDP}: {An} {Automated} and {Integrated} {Approach} for {Proving} {Differential} {Privacy} or {Finding} {Precise} {Counterexamples}},
	isbn = {978-1-4503-7089-9},
	shorttitle = {{CheckDP}},
	url = {https://dl.acm.org/doi/10.1145/3372297.3417282},
	doi = {10.1145/3372297.3417282},
	abstract = {We propose CheckDP, an automated and integrated approach for proving or disproving claims that a mechanism is differentially private. CheckDP can find counterexamples for mechanisms with subtle bugs for which prior counterexample generators have failed. Furthermore, it was able to automatically generate proofs for correct mechanisms for which no formal verification was reported before. CheckDP is built on static program analysis, allowing it to be more efficient and precise in catching infrequent events than sampling based counterexample generators (which run mechanisms hundreds of thousands of times to estimate their output distribution). Moreover, its sound approach also allows automatic verification of correct mechanisms. When evaluated on standard benchmarks and newer privacy mechanisms, CheckDP generates proofs (for correct mechanisms) and counterexamples (for incorrect mechanisms) within 70 seconds without any false positives or false negatives.},
	language = {en},
	urldate = {2020-12-30},
	booktitle = {Proceedings of the 2020 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Wang, Yuxin and Ding, Zeyu and Kifer, Daniel and Zhang, Danfeng},
	month = oct,
	year = {2020},
	pages = {919--938},
}

@article{chen_privacy_2015,
	title = {On the {Privacy} {Properties} of {Variants} on the {Sparse} {Vector} {Technique}},
	url = {http://arxiv.org/abs/1508.07306},
	abstract = {The sparse vector technique is a powerful diﬀerentially private primitive that allows an analyst to check whether queries in a stream are greater or lesser than a threshold. This technique has a unique property – the algorithm works by adding noise with a ﬁnite variance to the queries and the threshold, and guarantees privacy that only degrades with (a) the maximum sensitivity of any one query in stream, and (b) the number of positive answers output by the algorithm. Recent work has developed variants of this algorithm, which we call generalized private threshold testing, and are claimed to have privacy guarantees that do not depend on the number of positive or negative answers output by the algorithm. These algorithms result in a signiﬁcant improvement in utility over the sparse vector technique for a given privacy budget, and have found applications in frequent itemset mining, feature selection in machine learning and generating synthetic data.},
	language = {en},
	urldate = {2020-12-28},
	journal = {arXiv:1508.07306 [cs]},
	author = {Chen, Yan and Machanavajjhala, Ashwin},
	month = aug,
	year = {2015},
	note = {arXiv: 1508.07306},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Databases},
}

@article{lyu_understanding_2017,
	title = {Understanding the sparse vector technique for differential privacy},
	volume = {10},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3055330.3055331},
	doi = {10.14778/3055330.3055331},
	abstract = {The Sparse Vector Technique (SVT) is a fundamental technique for satisfying differential privacy and has the unique quality that one can output some query answers without apparently paying any privacy cost. SVT has been used in both the interactive setting, where one tries to answer a sequence of queries that are not known ahead of the time, and in the non-interactive setting, where all queries are known. Because of the potential savings on privacy budget, many variants for SVT have been proposed and employed in privacypreserving data mining and publishing. However, most variants of SVT are actually not private. In this paper, we analyze these errors and identify the misunderstandings that likely contribute to them. We also propose a new version of SVT that provides better utility, and introduce an effective technique to improve the performance of SVT. These enhancements can be applied to improve utility in the interactive setting. Through both analytical and experimental comparisons, we show that, in the non-interactive setting (but not the interactive setting), the SVT technique is unnecessary, as it can be replaced by the Exponential Mechanism (EM) with better accuracy.},
	language = {en},
	number = {6},
	urldate = {2020-12-28},
	journal = {Proceedings of the VLDB Endowment},
	author = {Lyu, Min and Su, Dong and Li, Ninghui},
	month = feb,
	year = {2017},
	pages = {637--648},
}

@article{dwork_concentrated_2016,
	title = {Concentrated {Differential} {Privacy}},
	url = {http://arxiv.org/abs/1603.01887},
	abstract = {We introduce Concentrated Diﬀerential Privacy, a relaxation of Diﬀerential Privacy enjoying better accuracy than both pure diﬀerential privacy and its popular “(ε, δ)” relaxation without compromising on cumulative privacy loss over multiple computations.},
	language = {en},
	urldate = {2020-12-28},
	journal = {arXiv:1603.01887 [cs]},
	author = {Dwork, Cynthia and Rothblum, Guy N.},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.01887},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Data Structures and Algorithms},
}

@inproceedings{mironov_renyi_2017,
	address = {Santa Barbara, CA},
	title = {Rényi {Differential} {Privacy}},
	isbn = {978-1-5386-3217-8},
	url = {https://ieeexplore.ieee.org/document/8049725/},
	doi = {10.1109/CSF.2017.11},
	abstract = {We propose a natural relaxation of differential privacy based on the Re´nyi divergence. Closely related notions have appeared in several recent papers that analyzed composition of differentially private mechanisms. We argue that the useful analytical tool can be used as a privacy deﬁnition, compactly and accurately representing guarantees on the tails of the privacy loss. We demonstrate that the new deﬁnition shares many important properties with the standard deﬁnition of differential privacy, while additionally allowing tighter analysis of composite heterogeneous mechanisms.},
	language = {en},
	urldate = {2020-12-27},
	booktitle = {2017 {IEEE} 30th {Computer} {Security} {Foundations} {Symposium} ({CSF})},
	publisher = {IEEE},
	author = {Mironov, Ilya},
	month = aug,
	year = {2017},
	pages = {263--275},
}

@article{truex_ldp-fed_2020,
	title = {{LDP}-{Fed}: {Federated} {Learning} with {Local} {Differential} {Privacy}},
	shorttitle = {{LDP}-{Fed}},
	url = {http://arxiv.org/abs/2006.03637},
	abstract = {This paper presents LDP-Fed, a novel federated learning system with a formal privacy guarantee using local differential privacy (LDP). Existing LDP protocols are developed primarily to ensure data privacy in the collection of single numerical or categorical values, such as click count in Web access logs. However, in federated learning model parameter updates are collected iteratively from each participant and consist of high dimensional, continuous values with high precision (10s of digits after the decimal point), making existing LDP protocols inapplicable. To address this challenge in LDP-Fed, we design and develop two novel approaches. First, LDP-Fed’s LDP Module provides a formal differential privacy guarantee for the repeated collection of model training parameters in the federated training of large-scale neural networks over multiple individual participantsâĂŹ private datasets. Second, LDP-Fed implements a suite of selection and filtering techniques for perturbing and sharing select parameter updates with the parameter server. We validate our system deployed with a condensed LDP protocol in training deep neural networks on public data. We compare this version of LDP-Fed, coined CLDP-Fed, with other state-of-the-art approaches with respect to model accuracy, privacy preservation, and system capabilities.},
	language = {en},
	urldate = {2020-12-27},
	journal = {arXiv:2006.03637 [cs, stat]},
	author = {Truex, Stacey and Liu, Ling and Chow, Ka-Ho and Gursoy, Mehmet Emre and Wei, Wenqi},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.03637},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{zhang_machine_2020,
	title = {Machine {Learning} {Testing}: {Survey}, {Landscapes} and {Horizons}},
	issn = {0098-5589, 1939-3520, 2326-3881},
	shorttitle = {Machine {Learning} {Testing}},
	url = {https://ieeexplore.ieee.org/document/9000651/},
	doi = {10.1109/TSE.2019.2962027},
	abstract = {This paper provides a comprehensive survey of Machine Learning Testing (ML testing) research. It covers 138 papers on testing properties (e.g., correctness, robustness, and fairness), testing components (e.g., the data, learning program, and framework), testing workﬂow (e.g., test generation and test evaluation), and application scenarios (e.g., autonomous driving, machine translation). The paper also analyses trends concerning datasets, research trends, and research focus, concluding with research challenges and promising research directions in ML testing.},
	language = {en},
	urldate = {2020-12-27},
	journal = {IEEE Transactions on Software Engineering},
	author = {Zhang, Jie M. and Harman, Mark and Ma, Lei and Liu, Yang},
	year = {2020},
	pages = {1--1},
}

@article{mcmahan_general_2019,
	title = {A {General} {Approach} to {Adding} {Differential} {Privacy} to {Iterative} {Training} {Procedures}},
	url = {http://arxiv.org/abs/1812.06210},
	abstract = {In this work we address the practical challenges of training machine learning models on privacy-sensitive datasets by introducing a modular approach that minimizes changes to training algorithms, provides a variety of conﬁguration strategies for the privacy mechanism, and then isolates and simpliﬁes the critical logic that computes the ﬁnal privacy guarantees. A key challenge is that training algorithms often require estimating many different quantities (vectors) from the same set of examples — for example, gradients of different layers in a deep learning architecture, as well as metrics and batch normalization parameters. Each of these may have different properties like dimensionality, magnitude, and tolerance to noise. By extending previous work on the Moments Accountant for the subsampled Gaussian mechanism, we can provide privacy for such heterogeneous sets of vectors, while also structuring the approach to minimize software engineering challenges.},
	language = {en},
	urldate = {2020-12-27},
	journal = {arXiv:1812.06210 [cs, stat]},
	author = {McMahan, H. Brendan and Andrew, Galen and Erlingsson, Ulfar and Chien, Steve and Mironov, Ilya and Papernot, Nicolas and Kairouz, Peter},
	month = mar,
	year = {2019},
	note = {arXiv: 1812.06210},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{balle_hypothesis_nodate,
	title = {Hypothesis {Testing} {Interpretations} and {Rényi} {Diﬀerential} {Privacy}},
	abstract = {Diﬀerential privacy is a de facto standard in data privacy, with applications in the public and private sectors. One way of explaining differential privacy that is particularly appealing to statistician and social scientists is through its statistical hypothesis testing interpretation. Informally, one cannot eﬀectively test whether a speciﬁc individual has contributed her data by observing the output of a private mechanism—no test can have both high signiﬁcance and high power. In this paper, we identify some conditions under which a privacy deﬁnition given in terms of a statistical divergence satisﬁes a similar interpretation. These conditions are useful to analyze the distinguishing power of divergences and we use them to study the hypothesis testing interpretation of relaxations of diﬀerential privacy based on Rényi divergence. Our analysis also results in an improved conversion rule between these deﬁnitions and diﬀerential privacy.},
	language = {en},
	author = {Balle, Borja and Barthe, Gilles and Gaboardi, Marco and Hsu, Justin and Sato, Tetsuya},
	pages = {10},
}

@inproceedings{wang_proving_2019,
	address = {Phoenix AZ USA},
	title = {Proving differential privacy with shadow execution},
	isbn = {978-1-4503-6712-7},
	url = {https://dl.acm.org/doi/10.1145/3314221.3314619},
	doi = {10.1145/3314221.3314619},
	abstract = {Recent work on formal verification of differential privacy shows a trend toward usability and expressiveness – generating a correctness proof of sophisticated algorithm while minimizing the annotation burden on programmers. Sometimes, combining those two requires substantial changes to program logics: one recent paper is able to verify Report Noisy Max automatically, but it involves a complex verification system using customized program logics and verifiers. In this paper, we propose a new proof technique, called shadow execution, and embed it into a language called ShadowDP. ShadowDP uses shadow execution to generate proofs of differential privacy with very few programmer annotations and without relying on customized logics and verifiers. In addition to verifying Report Noisy Max, we show that it can verify a new variant of Sparse Vector that reports the gap between some noisy query answers and the noisy threshold. Moreover, ShadowDP reduces the complexity of verification: for all of the algorithms we have evaluated, type checking and verification in total takes at most 3 seconds, while prior work takes minutes on the same algorithms.},
	language = {en},
	urldate = {2020-12-27},
	booktitle = {Proceedings of the 40th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Wang, Yuxin and Ding, Zeyu and Wang, Guanhong and Kifer, Daniel and Zhang, Danfeng},
	month = jun,
	year = {2019},
	pages = {655--669},
}

@inproceedings{liu_dependence_2016,
	address = {San Diego, CA},
	title = {Dependence {Makes} {You} {Vulnerable}: {Differential} {Privacy} {Under} {Dependent} {Tuples}},
	isbn = {978-1-891562-41-9},
	shorttitle = {Dependence {Makes} {You} {Vulnerable}},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2017/09/dependence-makes-you-vulnerable-differential-privacy-under-dependent-tuples.pdf},
	doi = {10.14722/ndss.2016.23279},
	abstract = {Differential privacy (DP) is a widely accepted mathematical framework for protecting data privacy. Simply stated, it guarantees that the distribution of query results changes only slightly due to the modiﬁcation of any one tuple in the database. This allows protection, even against powerful adversaries, who know the entire database except one tuple. For providing this guarantee, differential privacy mechanisms assume independence of tuples in the database – a vulnerable assumption that can lead to degradation in expected privacy levels especially when applied to real-world datasets that manifest natural dependence owing to various social, behavioral, and genetic relationships between users. In this paper, we make several contributions that not only demonstrate the feasibility of exploiting the above vulnerability but also provide steps towards mitigating it. First, we present an inference attack, using real datasets, where an adversary leverages the probabilistic dependence between tuples to extract users’ sensitive information from differentially private query results (violating the DP guarantees). Second, we introduce the notion of dependent differential privacy (DDP) that accounts for the dependence that exists between tuples and propose a dependent perturbation mechanism (DPM) to achieve the privacy guarantees in DDP. Finally, using a combination of theoretical analysis and extensive experiments involving different classes of queries (e.g., machine learning queries, graph queries) issued over multiple large-scale real-world datasets, we show that our DPM consistently outperforms state-of-the-art approaches in managing the privacy-utility tradeoffs for dependent data.},
	language = {en},
	urldate = {2020-12-23},
	booktitle = {Proceedings 2016 {Network} and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Liu, Changchang and Chakraborty, Supriyo and Mittal, Prateek},
	year = {2016},
}

@inproceedings{hitaj_deep_2017,
	address = {Dallas Texas USA},
	title = {Deep {Models} {Under} the {GAN}: {Information} {Leakage} from {Collaborative} {Deep} {Learning}},
	isbn = {978-1-4503-4946-8},
	shorttitle = {Deep {Models} {Under} the {GAN}},
	url = {https://dl.acm.org/doi/10.1145/3133956.3134012},
	doi = {10.1145/3133956.3134012},
	abstract = {Deep Learning has recently become hugely popular in machine learning for its ability to solve end-to-end learning systems, in which the features and the classifiers are learned simultaneously, providing significant improvements in classification accuracy in the presence of highly-structured and large databases.},
	language = {en},
	urldate = {2020-12-22},
	booktitle = {Proceedings of the 2017 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Hitaj, Briland and Ateniese, Giuseppe and Perez-Cruz, Fernando},
	month = oct,
	year = {2017},
	pages = {603--618},
}

@incollection{halevi_computational_2009,
	address = {Berlin, Heidelberg},
	title = {Computational {Differential} {Privacy}},
	volume = {5677},
	isbn = {978-3-642-03355-1 978-3-642-03356-8},
	url = {http://link.springer.com/10.1007/978-3-642-03356-8_8},
	abstract = {The deﬁnition of diﬀerential privacy has recently emerged as a leading standard of privacy guarantees for algorithms on statistical databases. We oﬀer several relaxations of the deﬁnition which require privacy guarantees to hold only against eﬃcient—i.e., computationallybounded—adversaries. We establish various relationships among these notions, and in doing so, we observe their close connection with the theory of pseudodense sets by Reingold et al. [1]. We extend the dense model theorem of Reingold et al. to demonstrate equivalence between two deﬁnitions (indistinguishability- and simulatability-based) of computational diﬀerential privacy.},
	language = {en},
	urldate = {2020-12-22},
	booktitle = {Advances in {Cryptology} - {CRYPTO} 2009},
	publisher = {Springer Berlin Heidelberg},
	author = {Mironov, Ilya and Pandey, Omkant and Reingold, Omer and Vadhan, Salil},
	editor = {Halevi, Shai},
	year = {2009},
	doi = {10.1007/978-3-642-03356-8_8},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {126--142},
}

@article{liu_minimax_2019,
	title = {Minimax {Rates} of {Estimating} {Approximate} {Differential} {Privacy}},
	url = {http://arxiv.org/abs/1905.10335},
	abstract = {Diﬀerential privacy has become a widely accepted notion of privacy, leading to the introduction and deployment of numerous privatization mechanisms. However, ensuring the privacy guarantee is an error-prone process, both in designing mechanisms and in implementing those mechanisms. Both types of errors will be greatly reduced, if we have a data-driven approach to verify privacy guarantees, from a black-box access to a mechanism. We pose it as a property estimation problem, and study the fundamental trade-oﬀs involved in the accuracy in estimated privacy guarantees and the number of samples required. We introduce a novel estimator that uses polynomial approximation of a carefully chosen degree to optimally trade-oﬀ bias and variance. With n samples, we show that this estimator achieves performance of a straightforward plug-in estimator with n ln n samples, a phenomenon referred to as eﬀective sample size ampliﬁcation. The minimax optimality of the proposed estimator is proved by comparing it to a matching fundamental lower bound.},
	language = {en},
	urldate = {2020-12-22},
	journal = {arXiv:1905.10335 [cs, math]},
	author = {Liu, Xiyang and Oh, Sewoong},
	month = may,
	year = {2019},
	note = {arXiv: 1905.10335},
	keywords = {Computer Science - Information Theory},
}

@inproceedings{abadi_deep_2016,
	address = {Vienna Austria},
	title = {Deep {Learning} with {Differential} {Privacy}},
	isbn = {978-1-4503-4139-4},
	url = {https://dl.acm.org/doi/10.1145/2976749.2978318},
	doi = {10.1145/2976749.2978318},
	abstract = {Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a reﬁned analysis of privacy costs within the framework of diﬀerential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training eﬃciency, and model quality.},
	language = {en},
	urldate = {2020-12-21},
	booktitle = {Proceedings of the 2016 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H. Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
	month = oct,
	year = {2016},
	pages = {308--318},
}

@incollection{gennaro_privacy_2015,
	address = {Berlin, Heidelberg},
	title = {Privacy with {Imperfect} {Randomness}},
	volume = {9216},
	isbn = {978-3-662-47999-5 978-3-662-48000-7},
	url = {http://link.springer.com/10.1007/978-3-662-48000-7_23},
	abstract = {We revisit the impossibility of a variety of cryptographic tasks including privacy and diﬀerential privacy with imperfect randomness. For traditional notions of privacy, such as security of encryption, commitment or secret sharing schemes, dramatic impossibility results are known [MP90,DOPS04] for several concrete sources R, including a (seemingly) very “nice and friendly” Santha-Vazirani (SV) source. Moreover, Bosley and Dodis [BD07] gave strong evidence that many traditional privacy tasks (e.g., encryption) inherently require an “extractable” source of randomness.},
	language = {en},
	urldate = {2020-12-21},
	booktitle = {Advances in {Cryptology} -- {CRYPTO} 2015},
	publisher = {Springer Berlin Heidelberg},
	author = {Dodis, Yevgeniy and Yao, Yanqing},
	editor = {Gennaro, Rosario and Robshaw, Matthew},
	year = {2015},
	doi = {10.1007/978-3-662-48000-7_23},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {463--482},
}

@incollection{safavi-naini_differential_2012,
	address = {Berlin, Heidelberg},
	title = {Differential {Privacy} with {Imperfect} {Randomness}},
	volume = {7417},
	isbn = {978-3-642-32008-8 978-3-642-32009-5},
	url = {http://link.springer.com/10.1007/978-3-642-32009-5_29},
	abstract = {In this work we revisit the question of basing cryptography on imperfect randomness. Bosley and Dodis (TCC’07) showed that if a source of randomness R is “good enough” to generate a secret key capable of encrypting k bits, then one can deterministically extract nearly k almost uniform bits from R, suggesting that traditional privacy notions (namely, indistinguishability of encryption) requires an “extractable” source of randomness. Other, even stronger impossibility results are known for achieving privacy under speciﬁc “non-extractable” sources of randomness, such as the γ-Santha-Vazirani (SV) source, where each next bit has fresh entropy, but is allowed to have a small bias γ {\textless} 1 (possibly depending on prior bits).},
	language = {en},
	urldate = {2020-12-21},
	booktitle = {Advances in {Cryptology} – {CRYPTO} 2012},
	publisher = {Springer Berlin Heidelberg},
	author = {Dodis, Yevgeniy and López-Alt, Adriana and Mironov, Ilya and Vadhan, Salil},
	editor = {Safavi-Naini, Reihaneh and Canetti, Ran},
	year = {2012},
	doi = {10.1007/978-3-642-32009-5_29},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {497--516},
}

@inproceedings{mironov_significance_2012,
	address = {Raleigh, North Carolina, USA},
	title = {On significance of the least significant bits for differential privacy},
	isbn = {978-1-4503-1651-4},
	url = {http://dl.acm.org/citation.cfm?doid=2382196.2382264},
	doi = {10.1145/2382196.2382264},
	abstract = {We describe a new type of vulnerability present in many implementations of diﬀerentially private mechanisms. In particular, all four publicly available general purpose systems for diﬀerentially private computations are susceptible to our attack.},
	language = {en},
	urldate = {2020-12-16},
	booktitle = {Proceedings of the 2012 {ACM} conference on {Computer} and communications security - {CCS} '12},
	publisher = {ACM Press},
	author = {Mironov, Ilya},
	year = {2012},
	pages = {650},
}

@article{mackay_information_nodate,
	title = {Information {Theory}, {Inference}, and {Learning} {Algorithms}},
	language = {en},
	author = {MacKay, David J C},
	pages = {640},
}

@article{garfinkel_randomness_2020,
	title = {Randomness {Concerns} {When} {Deploying} {Differential} {Privacy}},
	url = {http://arxiv.org/abs/2009.03777},
	doi = {10.1145/3411497.3420211},
	abstract = {The U.S. Census Bureau is using diﬀerential privacy (DP) to protect conﬁdential respondent data collected for the 2020 Decennial Census of Population \& Housing. The Census Bureau’s DP system is implemented in the Disclosure Avoidance System (DAS) and requires a source of random numbers. We estimate that the 2020 Census will require roughly 90TB of random bytes to protect the person and household tables. Although there are critical differences between cryptography and DP, they have similar requirements for randomness. We review the history of random number generation on deterministic computers. We also review hardware random number generator schemes, including the use of so-called “Lava Lamps” and the Intel Secure Key RDRAND instruction. We ﬁnally present our plan for generating random bits in the Amazon Web Services (AWS) environment using AES-CTR-DRBG seeded by mixing bits from /dev/urandom and the Intel Secure Key RDSEED instruction, a compromise of our desire to rely on a trusted hardware implementation, the unease of our external reviewers in trusting a hardware-only implementation, and the need to generate so many random bits.},
	language = {en},
	urldate = {2020-12-01},
	journal = {Proceedings of the 19th Workshop on Privacy in the Electronic Society},
	author = {Garfinkel, Simson L. and Leclerc, Philip},
	month = nov,
	year = {2020},
	note = {arXiv: 2009.03777},
	keywords = {Computer Science - Computers and Society, Computer Science - Cryptography and Security},
	pages = {73--86},
}

@article{alassad_combining_2020,
	title = {Combining advanced computational social science and graph theoretic techniques to reveal adversarial information operations},
	issn = {0306-4573},
	url = {http://www.sciencedirect.com/science/article/pii/S0306457320308803},
	doi = {10.1016/j.ipm.2020.102385},
	abstract = {Social media has influenced socio-political aspects of many societies around the world. It is an effortless way for people to enhance their communication, connect with like-minded people, and share ideas. Online social networks (OSNs) can be used for noble causes by bringing together communities with common shared interests and to promote awareness of various causes. However, there is a dark side to the use of OSNs. OSNs can also be used as a coordination and amplification platform for attacks. For instance, adversaries can increase the impact of an attack by causing panic in an area by promoting attacks using OSNs. Public data can help adversaries to determine the best timing for attacks, scheduling attacks, and then using OSNs to coordinate attacks on networks or physical locations. This convergence of the cyber and physical worlds is known as cybernetics. In this paper, we introduce an integrated method to identify malicious behavior and the actors responsible for propagating this behavior via online social networks. Throughout history we have used surveillance techniques to monitor negative behavior, activities, and information. Quantitative socio-technical methods such as deviant cyber flash mob (DCFM) detection and focal structure analysis (FSA) can provide reconnaissance capabilities that enable cities and governments to look beyond internal data and identify threats based on active events. Groups of powerful hackers can be identified through FSA which is an integrated model that uses a betweenness centrality method at the node-level and spectral modularity at group-level to identify a hidden malicious and powerful focal structure (a subset of the network). Assessment of groups using DCFM methods can help to identify powerful actors and prevent attacks. In this study, we examine multiple data sets integrating the DCFM and FSA models to help cybersecurity experts see a better picture of the threat which will help to plan a better response.},
	journal = {Information Processing \& Management},
	author = {Alassad, Mustafa and Spann, Billy and Agarwal, Nitin},
	month = sep,
	year = {2020},
	keywords = {Adversarial behaviors, Centrality-modularity, Deviant cyber flash mob detection, Disinformation, Focal structure analysis, Hub and spoke, Information operations, Misinformation, Social networks},
	pages = {102385},
}

@article{zarocostas_how_2020,
	title = {How to fight an infodemic},
	volume = {395},
	issn = {0140-6736},
	url = {https://doi.org/10.1016/S0140-6736(20)30461-X},
	doi = {10.1016/S0140-6736(20)30461-X},
	number = {10225},
	urldate = {2020-11-22},
	journal = {The Lancet},
	author = {Zarocostas, John},
	month = feb,
	year = {2020},
	note = {Publisher: Elsevier},
	pages = {676},
}

@article{milanovic_cyber_2020,
	title = {Cyber {Attacks} and {Cyber} ({Mis})information {Operations} during a {Pandemic}},
	issn = {1556-5068},
	url = {https://www.ssrn.com/abstract=3612019},
	doi = {10.2139/ssrn.3612019},
	language = {en},
	urldate = {2020-11-22},
	journal = {SSRN Electronic Journal},
	author = {Milanovic, Marko and Schmitt, Michael N.},
	year = {2020},
}

@article{weedon_information_2017,
	title = {Information {Operations} and {Facebook}},
	url = {https://www.mm.dk/wp-content/uploads/2017/05/facebook-and-information-operations-v1.pdf},
	language = {en},
	author = {Weedon, Jen and Nuland, William and Stamos, Alex},
	year = {2017},
	pages = {13},
}

@article{arif_acting_nodate,
	title = {Acting the {Part}: {Examining} {Information} {Operations} {Within} \#{BlackLivesMatter} {Discourse}},
	volume = {2},
	language = {en},
	author = {Arif, Ahmer and Stewart, Leo G and Starbird, Kate},
	pages = {27},
}

@article{crosston_world_2011,
	title = {World {Gone} {Cyber} {MAD}},
	language = {en},
	author = {Crosston, Matthew D},
	year = {2011},
	pages = {17},
}

@inproceedings{brantly_cyber_2018,
	address = {Tallinn},
	title = {The cyber deterrence problem},
	isbn = {978-9949-9904-2-9 978-9949-9904-3-6},
	url = {https://ieeexplore.ieee.org/document/8405009/},
	doi = {10.23919/CYCON.2018.8405009},
	abstract = {What is the role of deterrence in an age where adept hackers can credibly hold strategic assets at risk? Do conventional frameworks of deterrence maintain their applicability and meaning against state actors in cyberspace? Is it possible to demonstrate credibility with either in-domain or cross-domain signaling or is cyberspace fundamentally ill-suited to the application of deterrence frameworks? Building on concepts from both rational deterrence theory and cognitive theories of deterrence this work attempts to leverage relevant examples from both within and beyond cyberspace to examine applicability of deterrence in the digital age and for digital tools in an effort to shift the conversation from Atoms to Bits and Bytes.},
	language = {en},
	urldate = {2020-11-22},
	booktitle = {2018 10th {International} {Conference} on {Cyber} {Conflict} ({CyCon})},
	publisher = {IEEE},
	author = {Brantly, Aaron F.},
	month = may,
	year = {2018},
	pages = {31--54},
}

@book{kramer_cyberpower_2011,
	title = {Cyberpower and {National} {Security}},
	isbn = {978-1-59797-933-7 978-1-59797-423-3},
	url = {http://www.jstor.org/stable/10.2307/j.ctt1djmhj1},
	language = {en},
	urldate = {2020-11-22},
	publisher = {Potomac Books},
	editor = {Kramer, Franklin D. and Starr, Stuart H. and Wentz, Larry K.},
	month = mar,
	year = {2011},
	doi = {10.2307/j.ctt1djmhj1},
}

@article{iasiello_is_2014,
	title = {Is {Cyber} {Deterrence} an {Illusory} {Course} of {Action}?},
	volume = {7},
	issn = {1944-0464, 1944-0472},
	url = {http://scholarcommons.usf.edu/jss/vol7/iss1/6/},
	doi = {10.5038/1944-0472.7.1.5},
	abstract = {With the U.S. government acknowledgement of the seriousness of cyber threats, particularly against its critical infrastructures, as well as the Department of Defense officially labeling cyberspace as a war fighting domain, the Cold War strategy of deterrence is being applied to the cyber domain. However, unlike the nuclear realm, cyber deterrence must incorporate a wide spectrum of potential adversaries of various skill, determination, and capability, ranging from individual actors to state run enterprises. What’s more, the very principles that achieved success in deterring the launch of nuclear weapons during the Cold War, namely the threat of severe retaliation, cannot be achieved in cyberspace, thus neutralizing the potential effectiveness of leveraging a similar strategy. Attribution challenges, the ability to respond quickly and effectively, and the ability to sustain a model of repeatability prove to be insurmountable in a domain where actors operate in obfuscation.},
	language = {en},
	number = {1},
	urldate = {2020-11-22},
	journal = {Journal of Strategic Security},
	author = {Iasiello, Emilio},
	month = mar,
	year = {2014},
	pages = {54--67},
}

@article{tor_cumulative_2017,
	title = {‘{Cumulative} {Deterrence}’ as a {New} {Paradigm} for {Cyber} {Deterrence}},
	volume = {40},
	issn = {0140-2390, 1743-937X},
	url = {https://www.tandfonline.com/doi/full/10.1080/01402390.2015.1115975},
	doi = {10.1080/01402390.2015.1115975},
	abstract = {This article suggests that there is a paradigm crisis in the sub-ﬁeld of cyber deterrence. Cyber deterrence is evolving slowly and unpromisingly as a strategic tool in both theory and practice, mostly due to the ill-ﬁtting theoretical framework and underlining assumptions it borrows from the absolute-nuclear-deterrence context. Therefore, this article suggests replacing the accepted yet inadequate paradigm of absolute deterrence with a better-ﬁtting restrictivecumulative-deterrence paradigm that draws on the Israeli approach to deterrence, introducing it into the cyber domain. The article further criticizes the current discourse in the ﬁeld, including some ‘common knowledge’ (mis)understandings of cyberspace and the ways it aﬀects the possibility of deterrence.},
	language = {en},
	number = {1-2},
	urldate = {2020-11-22},
	journal = {Journal of Strategic Studies},
	author = {Tor, Uri},
	month = jan,
	year = {2017},
	pages = {92--117},
}

@article{stone_cyber_2013,
	title = {Cyber {War} {Will} {Take} {Place}!},
	volume = {36},
	issn = {0140-2390, 1743-937X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01402390.2012.730485},
	doi = {10.1080/01402390.2012.730485},
	abstract = {The question of whether or not cyber war amounts to war per se is difﬁcult to determine given strategic theory’s uncertain grasp of the concepts of force, violence and lethality. These three concepts, along with their relationships with one another, are explored in order to demonstrate that cyber attacks can in fact be construed as acts of war.},
	language = {en},
	number = {1},
	urldate = {2020-11-20},
	journal = {Journal of Strategic Studies},
	author = {Stone, John},
	month = feb,
	year = {2013},
	pages = {101--108},
}

@book{libicki_cyberdeterrence_2009,
	address = {Santa Monica, CA},
	title = {Cyberdeterrence and cyberwar},
	isbn = {978-0-8330-4734-2},
	abstract = {Cyberspace, where information--and hence serious value--is stored and manipulated, is a tempting target. An attacker could be a person, group, or state and may disrupt or corrupt the systems from which cyberspace is built. When states are involved, it is tempting to compare fights to warfare, but there are important differences. The author addresses these differences and ways the United States protect itself in the face of attack},
	language = {en},
	publisher = {RAND},
	author = {Libicki, Martin C.},
	collaborator = {Project Air Force (U.S.)},
	year = {2009},
	note = {OCLC: ocn428819545},
	keywords = {Civil defense, Computer networks, Cyberspace, Cyberspace operations (Military science), Cyberterrorism, Information warfare, Prevention, Security measures, United States},
}

@article{rid_cyber_2012,
	title = {Cyber {War} {Will} {Not} {Take} {Place}},
	volume = {35},
	issn = {0140-2390, 1743-937X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01402390.2011.608939},
	doi = {10.1080/01402390.2011.608939},
	abstract = {For almost two decades, experts and defense establishments the world over have been predicting that cyber war is coming. But is it? This article argues in three steps that cyber war has never happened in the past, that cyber war does not take place in the present, and that it is unlikely that cyber war will occur in the future. It ﬁrst outlines what would constitute cyber war: a potentially lethal, instrumental, and political act of force conducted through malicious code. The second part shows what cyber war is not, case-by-case. Not one single cyber offense on record constitutes an act of war on its own. The ﬁnal part offers a more nuanced terminology to come to terms with cyber attacks. All politically motivated cyber attacks are merely sophisticated versions of three activities that are as old as warfare itself: sabotage, espionage, and subversion.},
	language = {en},
	number = {1},
	urldate = {2020-11-19},
	journal = {Journal of Strategic Studies},
	author = {Rid, Thomas},
	month = feb,
	year = {2012},
	pages = {5--32},
}

@article{gordon_economics_nodate,
	title = {The {Economics} of {Information} {Security} {Investment}},
	volume = {5},
	language = {en},
	number = {4},
	journal = {ACM Transactions on Information and System Security},
	author = {Gordon, Lawrence A and Loeb, Martin P},
	pages = {20},
}

@inproceedings{truex_hybrid_2019,
	address = {London, United Kingdom},
	title = {A {Hybrid} {Approach} to {Privacy}-{Preserving} {Federated} {Learning}},
	isbn = {978-1-4503-6833-9},
	url = {http://dl.acm.org/citation.cfm?doid=3338501.3357370},
	doi = {10.1145/3338501.3357370},
	abstract = {Federated learning facilitates the collaborative training of models without the sharing of raw data. However, recent attacks demonstrate that simply maintaining data locality during training processes does not provide sufficient privacy guarantees. Rather, we need a federated learning system capable of preventing inference over both the messages exchanged during training and the final trained model while ensuring the resulting model also has acceptable predictive accuracy. Existing federated learning approaches either use secure multiparty computation (SMC) which is vulnerable to inference or differential privacy which can lead to low accuracy given a large number of parties with relatively small amounts of data each. In this paper, we present an alternative approach that utilizes both differential privacy and SMC to balance these trade-offs. Combining differential privacy with secure multiparty computation enables us to reduce the growth of noise injection as the number of parties increases without sacrificing privacy while maintaining a pre-defined rate of trust. Our system is therefore a scalable approach that protects against inference threats and produces models with high accuracy. Additionally, our system can be used to train a variety of machine learning models, which we validate with experimental results on 3 different machine learning algorithms. Our experiments demonstrate that our approach out-performs state of the art solutions.},
	language = {en},
	urldate = {2020-11-19},
	booktitle = {Proceedings of the 12th {ACM} {Workshop} on {Artificial} {Intelligence} and {Security}  - {AISec}'19},
	publisher = {ACM Press},
	author = {Truex, Stacey and Baracaldo, Nathalie and Anwar, Ali and Steinke, Thomas and Ludwig, Heiko and Zhang, Rui and Zhou, Yi},
	year = {2019},
	pages = {1--11},
}

@article{carlini_secret_nodate,
	title = {The {Secret} {Sharer}: {Evaluating} and {Testing} {Unintended} {Memorization} in {Neural} {Networks}},
	abstract = {This paper describes a testing methodology for quantitatively assessing the risk that rare or unique training-data sequences are unintentionally memorized by generative sequence models—a common type of machine-learning model. Because such models are sometimes trained on sensitive data (e.g., the text of users’ private messages), this methodology can beneﬁt privacy by allowing deep-learning practitioners to select means of training that minimize such memorization.},
	language = {en},
	author = {Carlini, Nicholas and Liu, Chang and Erlingsson, Úlfar and Kos, Jernej and Song, Dawn},
	pages = {19},
}

@article{jayaraman_evaluating_nodate,
	title = {Evaluating {Diﬀerentially} {Private} {Machine} {Learning} in {Practice}},
	abstract = {Diﬀerential privacy is a strong notion for privacy that can be used to prove formal guarantees, in terms of a privacy budget, , about how much information is leaked by a mechanism. When used in privacy-preserving machine learning, the goal is typically to limit what can be inferred from the model about individual training records. However, the calibration of the privacy budget is not well understood. Implementations of privacy-preserving machine learning often select large values of in order to get acceptable utility of the model, with little understanding of the impact of such choices on meaningful privacy. Moreover, in scenarios where iterative learning procedures are used, relaxed deﬁnitions of diﬀerential privacy are often used which appear to reduce the needed privacy budget but present poorly understood trade-oﬀs between privacy and utility. In this paper, we quantify the impact of these choices on privacy in experiments with logistic regression and neural network models. Our main ﬁnding is that there is no way to obtain privacy for free—relaxed deﬁnitions of diﬀerential privacy that reduce the amount of noise needed to improve utility also increase the measured privacy leakage. Current mechanisms for diﬀerentially private machine learning rarely oﬀer acceptable utility-privacy trade-oﬀs for complex learning tasks: settings that provide limited accuracy loss provide little eﬀective privacy, and settings that provide strong privacy result in useless models.},
	language = {en},
	author = {Jayaraman, Bargav and Evans, David},
	pages = {19},
}

@article{rogaway_moral_nodate,
	title = {The {Moral} {Character} of {Cryptographic} {Work}},
	abstract = {Cryptography rearranges power: it conﬁgures who can do what, from what. This makes cryptography an inherently political tool, and it confers on the ﬁeld an intrinsically moral dimension. The Snowden revelations motivate a reassessment of the political and moral positioning of cryptography. They lead one to ask if our inability to eﬀectively address mass surveillance constitutes a failure of our ﬁeld. I believe that it does. I call for a community-wide eﬀort to develop more eﬀective means to resist mass surveillance. I plead for a reinvention of our disciplinary culture to attend not only to puzzles and math, but, also, to the societal implications of our work.},
	language = {en},
	author = {Rogaway, Phillip},
	pages = {48},
}

@inproceedings{song_membership_2019,
	address = {San Francisco, CA, USA},
	title = {Membership {Inference} {Attacks} {Against} {Adversarially} {Robust} {Deep} {Learning} {Models}},
	isbn = {978-1-72813-508-3},
	url = {https://ieeexplore.ieee.org/document/8844607/},
	doi = {10.1109/SPW.2019.00021},
	abstract = {In recent years, the research community has increasingly focused on understanding the security and privacy challenges posed by deep learning models. However, the security domain and the privacy domain have typically been considered separately. It is thus unclear whether the defense methods in one domain will have any unexpected impact on the other domain. In this paper, we take a step towards enhancing our understanding of deep learning models when the two domains are combined together. We do this by measuring the success of membership inference attacks against two state-of-the-art adversarial defense methods that mitigate evasion attacks: adversarial training and provable defense. On the one hand, membership inference attacks aim to infer an individual’s participation in the target model’s training dataset and are known to be correlated with target model’s overﬁtting. On the other hand, adversarial defense methods aim to enhance the robustness of target models by ensuring that model predictions are unchanged for a small area around each sample in the training dataset. Intuitively, adversarial defenses may rely more on the training dataset and be more vulnerable to membership inference attacks. By performing empirical membership inference attacks on both adversarially robust models and corresponding undefended models, we ﬁnd that the adversarial training method is indeed more susceptible to membership inference attacks, and the privacy leakage is directly correlated with model robustness. We also ﬁnd that the provable defense approach does not lead to enhanced success of membership inference attacks. However, this is achieved by signiﬁcantly sacriﬁcing the accuracy of the model on benign data points, indicating that privacy, security, and prediction accuracy are not jointly achieved in these two approaches.},
	language = {en},
	urldate = {2020-11-17},
	booktitle = {2019 {IEEE} {Security} and {Privacy} {Workshops} ({SPW})},
	publisher = {IEEE},
	author = {Song, Liwei and Shokri, Reza and Mittal, Prateek},
	month = may,
	year = {2019},
	pages = {50--56},
}

@article{shokri_membership_2017,
	title = {Membership {Inference} {Attacks} against {Machine} {Learning} {Models}},
	url = {http://arxiv.org/abs/1610.05820},
	abstract = {We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model’s training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model’s predictions on the inputs that it trained on versus the inputs that it did not train on.},
	language = {en},
	urldate = {2020-11-17},
	journal = {arXiv:1610.05820 [cs, stat]},
	author = {Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
	month = mar,
	year = {2017},
	note = {arXiv: 1610.05820},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{noauthor_differential-privacy-1_nodate,
	title = {differential-privacy-1},
	language = {en},
	pages = {33},
}

@article{noauthor_differential-privacy-2_nodate,
	title = {differential-privacy-2},
	language = {en},
	pages = {23},
}

@article{pym_philosophy_nodate,
	title = {Philosophy, {Politics}, and {Economics} of {Security} and {Privacy} - {Lecture} 4},
	language = {en},
	author = {Pym, David},
	pages = {74},
}

@article{caulfield_philosophy_nodate,
	title = {Philosophy, {Politics}, and {Economics} of {Security} and {Privacy} - {Lecture} 5},
	language = {en},
	author = {Caulfield, Tristan and Pym, David},
	pages = {85},
}

@inproceedings{ding_detecting_2018,
	address = {Toronto Canada},
	title = {Detecting {Violations} of {Differential} {Privacy}},
	isbn = {978-1-4503-5693-0},
	url = {https://dl.acm.org/doi/10.1145/3243734.3243818},
	doi = {10.1145/3243734.3243818},
	abstract = {The widespread acceptance of differential privacy has led to the publication of many sophisticated algorithms for protecting privacy. However, due to the subtle nature of this privacy definition, many such algorithms have bugs that make them violate their claimed privacy. In this paper, we consider the problem of producing counterexamples for such incorrect algorithms. The counterexamples are designed to be short and human-understandable so that the counterexample generator can be used in the development process – a developer could quickly explore variations of an algorithm and investigate where they break down. Our approach is statistical in nature. It runs a candidate algorithm many times and uses statistical tests to try to detect violations of differential privacy. An evaluation on a variety of incorrect published algorithms validates the usefulness of our approach: it correctly rejects incorrect algorithms and provides counterexamples for them within a few seconds.},
	language = {en},
	urldate = {2020-11-14},
	booktitle = {Proceedings of the 2018 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Ding, Zeyu and Wang, Yuxin and Wang, Guanhong and Zhang, Danfeng and Kifer, Daniel},
	month = jan,
	year = {2018},
	pages = {475--489},
}

@inproceedings{kifer_no_2011,
	address = {Athens, Greece},
	title = {No free lunch in data privacy},
	isbn = {978-1-4503-0661-4},
	url = {http://portal.acm.org/citation.cfm?doid=1989323.1989345},
	doi = {10.1145/1989323.1989345},
	abstract = {Diﬀerential privacy is a powerful tool for providing privacypreserving noisy query answers over statistical databases. It guarantees that the distribution of noisy query answers changes very little with the addition or deletion of any tuple. It is frequently accompanied by popularized claims that it provides privacy without any assumptions about the data and that it protects against attackers who know all but one record. In this paper we critically analyze the privacy protections oﬀered by diﬀerential privacy.},
	language = {en},
	urldate = {2020-11-11},
	booktitle = {Proceedings of the 2011 international conference on {Management} of data - {SIGMOD} '11},
	publisher = {ACM Press},
	author = {Kifer, Daniel and Machanavajjhala, Ashwin},
	year = {2011},
	pages = {193},
}

@book{frodeman_doctoral_2017,
	title = {Doctoral {Student} and {Early} {Career} {Academic} {Perspectives} on {Interdisciplinarity}},
	volume = {1},
	url = {http://oxfordhandbooks.com/view/10.1093/oxfordhb/9780198733522.001.0001/oxfordhb-9780198733522-e-46},
	abstract = {This chapter focuses on the experiences of graduate students and early career academics who conduct interdisciplinary research and teaching. It offers practical advice to guide multiple participants (individuals, departments, institutions, disciplines) to navigate the benefits and challenges of participating in interdisciplinary research and teaching. The chapter concludes with reflections on the increasing calls for interdisciplinarity in the context of reduced funding for public universities and the rise of interdisciplinary programs at private liberal arts colleges.},
	language = {en},
	urldate = {2020-11-10},
	publisher = {Oxford University Press},
	author = {Dooling, Sarah and Graybill, Jessica K. and Shandas, Vivek},
	editor = {Frodeman, Robert},
	month = mar,
	year = {2017},
	doi = {10.1093/oxfordhb/9780198733522.013.46},
}

@article{meiser_approximate_nodate,
	title = {Approximate and {Probabilistic} {Diﬀerential} {Privacy} {Deﬁnitions}},
	abstract = {This technical report discusses three subtleties related to the widely used notion of diﬀerential privacy (DP). First, we discuss how the choice of a distinguisher inﬂuences the privacy notion and why we should always have a distinguisher if we consider approximate DP. Secondly, we draw a line between the very intuitive probabilistic diﬀerential privacy (with probability 1 − δ we have ε-DP) and the commonly used approximate diﬀerential privacy ((ε, δ)-DP). Finally we see that and why probabilistic diﬀerential privacy (and similar notions) are not complete under post-processing, which has signiﬁcant implications for notions used in the literature.},
	language = {en},
	author = {Meiser, Sebastian},
	keywords = {to read},
	pages = {9},
}

@incollection{domingo-ferrer_disclosure_2014,
	address = {Cham},
	title = {Disclosure {Risk} {Evaluation} for {Fully} {Synthetic} {Categorical} {Data}},
	volume = {8744},
	isbn = {978-3-319-11256-5 978-3-319-11257-2},
	url = {http://link.springer.com/10.1007/978-3-319-11257-2_15},
	abstract = {We present an approach for evaluating disclosure risks for fully synthetic categorical data. The basic idea is to compute probability distributions of unknown conﬁdential data values given the synthetic data and assumptions about intruder knowledge. We use a “worst-case” scenario of an intruder knowing all but one of the records in the conﬁdential data. To create the synthetic data, we use a Dirichlet process mixture of products of multinomial distributions, which is a Bayesian version of a latent class model. In addition to generating synthetic data with high utility, the likelihood function admits simple and convenient approximations to the disclosure risk probabilities via importance sampling. We illustrate the disclosure risk computations by synthesizing a subset of data from the American Community Survey.},
	language = {en},
	urldate = {2020-11-04},
	booktitle = {Privacy in {Statistical} {Databases}},
	publisher = {Springer International Publishing},
	author = {Hu, Jingchen and Reiter, Jerome P. and Wang, Quanli},
	editor = {Domingo-Ferrer, Josep},
	year = {2014},
	doi = {10.1007/978-3-319-11257-2_15},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {185--199},
}

@incollection{hutchison_framework_2012,
	address = {Berlin, Heidelberg},
	title = {A {Framework} for {Modelling} {Security} {Architectures} in {Services} {Ecosystems}},
	volume = {7592},
	isbn = {978-3-642-33426-9 978-3-642-33427-6},
	url = {http://link.springer.com/10.1007/978-3-642-33427-6_5},
	abstract = {We develop a compositional framework for modelling security and business architectures based on rigorous underlying mathematical systems modelling technology. We explain the basic architectural model, which strictly separates declarative speciﬁcation from operational implementation, and show architectures can interact by composition, substitution, and stacking. We illustrate these constructions using a running example based on airport security and an example based on (cloud-based) outsourcing, indicating how our approach can illustrate how security controls can fail or be circumvented in these cases. We explain our motivations from mathematical modelling and security economics, and conclude by indicating how to aim to develop a decision-support technology.},
	language = {en},
	urldate = {2020-11-02},
	booktitle = {Service-{Oriented} and {Cloud} {Computing}},
	publisher = {Springer Berlin Heidelberg},
	author = {Collinson, Matthew and Pym, David and Taylor, Barry},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and De Paoli, Flavio and Pimentel, Ernesto and Zavattaro, Gianluigi},
	year = {2012},
	doi = {10.1007/978-3-642-33427-6_5},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {64--79},
}

@article{yu_differentially_2018,
	title = {Differentially {Private} {Veriﬁcation} of {Regression} {Predictions} from {Synthetic} {Data}},
	abstract = {One approach for releasing public use ﬁles is to make synthetic data, i.e., data simulated from statistical models estimated on the conﬁdential data. Given access only to synthetic data, users cannot tell whether the synthetic data have been constructed in ways that provide sufﬁcient accuracy for their particular purposes. To enable users to make such assessments, data providers also can allow users to request veriﬁcation measures. These are summary statistics reﬂecting comparisons of the results of analysis based on the synthetic and conﬁdential data. We present three veriﬁcation measures that satisfy differential privacy for assessing the quality of linear regression models. We use simulation studies to illustrate the veriﬁcation measures.},
	language = {en},
	author = {Yu, Haoyang and Reiter, Jerome P},
	year = {2018},
	pages = {19},
}

@incollection{barthe_verified_2013,
	address = {New York},
	title = {Verified {Computational} {Differential} {Privacy} with {Applications} to {Smart} {Metering}},
	isbn = {978-0-7695-5031-2},
	abstract = {EasyCrypt is a tool-assisted framework for reasoning about probabilistic computations in the presence of adversarial code, whose main application has been the verification of security properties of cryptographic constructions in the computational model. We report on a significantly enhanced version of EasyCrypt that accommodates a richer, user-extensible language of probabilistic expressions and, more fundamentally, supports reasoning about approximate forms of program equivalence. This enhanced framework allows us to express a broader range of security properties, that notably include approximate and computational differential privacy. We illustrate the use of the framework by verifying two protocols: a two-party protocol for computing the Hamming distance between bit-vectors, yielding two-sided privacy guarantees; and a novel, efficient, and privacy-friendly distributed protocol to aggregate smart meter readings into statistics and bills.},
	language = {English},
	booktitle = {2013 {Ieee} 26th {Computer} {Security} {Foundations} {Symposium} (csf)},
	publisher = {Ieee},
	author = {Barthe, Gilles and Danezis, George and Gregoire, Benjamin and Kunz, Cesar and Zanella-Beguelin, Santiago},
	year = {2013},
	doi = {10.1109/CSF.2013.26},
	note = {ISSN: 1063-6900
WOS:000335225600019},
	keywords = {to read},
	pages = {287--301},
}

@article{kroll_accountable_2017,
	title = {Accountable {Algorithms}},
	volume = {165},
	issn = {0041-9907},
	abstract = {Many important decisions historically made by people are now made by computers. Algorithms count votes, approve loan and credit card applications, target citizens or neighborhoods for police scrutiny, select taxpayers for IRS audit, grant or deny immigration visas, and more. The accountability mechanisms and legal standards that govern such decision processes have not kept pace with technology. The tools currently available to policymakers, legislators, and courts were developed to oversee human decisionmakers and often fail when applied to computers instead. For example, how do you judge the intent of a piece of software? Because automated decision systems can return potentially incorrect, unjustified, or unfair results, additional approaches are needed to make such systems accountable and governable. This Article reveals a new technological toolkit to verify that automated decisions comply with key standards of legal fairness. We challenge the dominant position in the legal literature that transparency will solve these problems. Disclosure of source code is often neither necessary (because of alternative techniques from computer science) nor sufficient (because of the issues analyzing code) to demonstrate the fairness of a process. Furthermore, transparency may be undesirable, such as when it discloses private information or permits tax cheats or terrorists to game the systems determining audits or security screening. The central issue is how to assure the interests of citizens, and society as a whole, in making these processes more accountable. This Article argues that technology is creating new opportunities-subtler and more flexible than total transparency-to design decisionmaking algorithms so that they better align with legal and policy objectives. Doing so will improve not only the current governance of automated decisions, but also-in certain cases-the governance of decisionmaking in general. The implicit (or explicit) biases of human decisionmakers can be difficult to find and root out, but we can peer into the "brain" of an algorithm: computational processes and purpose specifications can be declared prior to use and verified afterward. The technological tools introduced in this Article apply widely. They can be used in designing decisionmaking processes from both the private and public sectors, and they can be tailored to verify different characteristics as desired by decisionmakers, regulators, or the public. By forcing a more careful consideration of the effects of decision rules, they also engender policy discussions and closer looks at legal standards. As such, these tools have far-reaching implications throughout law and society. Part I of this Article provides an accessible and concise introduction to foundational computer science techniques that can be used to verify and demonstrate compliance with key standards of legal fairness for automated decisions without revealing key attributes of the decisions or the processes by which the decisions were reached. Part II then describes how these techniques can assure that decisions are made with the key governance attribute of procedural regularity, meaning that decisions are made under an announced set of rules consistently applied in each case. We demonstrate how this approach could be used to redesign and resolve issues with the State Department's diversity visa lottery. In Part III, we go further and explore how other computational techniques can assure that automated decisions preserve fidelity to substantive legal and policy choices. We show how these tools may be used to assure that certain kinds of unjust discrimination are avoided and that automated decision processes behave in ways that comport with the social or legal standards that govern the decision. We also show how automated decisionmaking may even complicate existing doctrines of disparate treatment and disparate impact, and we discuss some recent computer science work on detecting and removing discrimination in algorithms, especially in the context of big data and machine learning. And lastly, in Part IV, we propose an agenda to further synergistic collaboration between computer science, law, and policy to advance the design of automated decision processes for accountability.},
	language = {English},
	number = {3},
	journal = {University of Pennsylvania Law Review},
	author = {Kroll, Joshua A. and Huey, Joanna and Barocas, Solon and Felten, Edward W. and Reidenberg, Joel R. and Robinson, David G. and Yu, Harlan},
	month = feb,
	year = {2017},
	note = {Place: Philadelphia
Publisher: Univ Penn Law Sch
WOS:000397048900003},
	keywords = {design, failure, internet, to read},
	pages = {633--705},
}

@article{toledo_lower-cost_2016,
	title = {Lower-{Cost} ∈-{Private} {Information} {Retrieval}},
	volume = {2016},
	issn = {2299-0984},
	url = {http://content.sciendo.com/view/journals/popets/2016/4/article-p184.xml},
	doi = {10.1515/popets-2016-0035},
	abstract = {Private Information Retrieval (PIR), despite being well studied, is computationally costly and arduous to scale. We explore lower-cost relaxations of information-theoretic PIR, based on dummy queries, sparse vectors, and compositions with an anonymity system. We prove the security of each scheme using a ﬂexible differentially private deﬁnition for private queries that can capture notions of imperfect privacy. We show that basic schemes are weak, but some of them can be made arbitrarily safe by composing them with large anonymity systems.},
	language = {en},
	number = {4},
	urldate = {2020-10-28},
	journal = {Proceedings on Privacy Enhancing Technologies},
	author = {Toledo, Raphael R. and Danezis, George and Goldberg, Ian},
	month = oct,
	year = {2016},
	pages = {184--201},
}

@incollection{dingledine_investments_2009,
	address = {Berlin, Heidelberg},
	title = {Investments and {Trade}-offs in the {Economics} of {Information} {Security}},
	volume = {5628},
	isbn = {978-3-642-03548-7 978-3-642-03549-4},
	url = {http://link.springer.com/10.1007/978-3-642-03549-4_9},
	abstract = {We develop and simulate a dynamic model of investment in information security. The model is based on the recognition that both IT managers and users appreciate the trade-oﬀ between two of the fundamental characteristics of information security, namely conﬁdentiality and availability. The model’s parameters can be clustered in a manner that allows us to categorize and compare the responses to shocks of various types of organizations. We derive the system’s stability conditions and ﬁnd that they admit a wide choice of parameters. We examine the system’s responses to the same shock in conﬁdentiality under diﬀerent parameter constellations that correspond to various types of organizations. Our analysis illustrates that the response to investments in information security will be uniform in neither size nor time evolution.},
	language = {en},
	urldate = {2020-10-26},
	booktitle = {Financial {Cryptography} and {Data} {Security}},
	publisher = {Springer Berlin Heidelberg},
	author = {Ioannidis, Christos and Pym, David and Williams, Julian},
	editor = {Dingledine, Roger and Golle, Philippe},
	year = {2009},
	doi = {10.1007/978-3-642-03549-4_9},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {148--166},
}

@article{ioannidis_is_2016,
	title = {Is {Public} {Co}-{Ordination} of {Investment} in {Information} {Security} {Desirable}?},
	volume = {07},
	issn = {2153-1234, 2153-1242},
	url = {http://dx.doi.org/10.4236/jis.2016.72005},
	doi = {10.4236/jis.2016.72005},
	abstract = {This paper provides for the presentation, in an integrated manner, of a sequence of results addressing the consequences of the presence of an information steward in an ecosystem under attack and establishes the appropriate defensive investment responses, thus allowing for a cohesive understanding of the nature of the information steward in a variety of attack contexts. We determine the level of investment in information security and attacking intensity when agents react in a non-coordinated manner and compare them to the case of the system’s coordinated response undertaken under the guidance of a steward. We show that only in the most well-designed institutional set-up the presence of the well-informed steward provides for an increase of the system’s resilience to attacks. In the case in which both the information available to the steward and its policy instruments are curtailed, coordinated policy responses yield no additional benefits to individual agents and in some case they actually compared unfavourably to atomistic responses. The system’s sustainability does improve in the presence of a steward, which deters attackers and reduces the numbers and intensity of attacks. In most cases, the resulting investment expenditure undertaken by the agents in the ecosystem exceeds its Pareto efficient magnitude.},
	language = {en},
	number = {02},
	urldate = {2020-10-25},
	journal = {Journal of Information Security},
	author = {Ioannidis, Christos and Pym, David and Williams, Julian},
	year = {2016},
	pages = {60--80},
}

@article{caulfield_philosophy_nodate,
	title = {Philosophy, {Politics}, and {Economics} of {Security} and {Privacy} - {Lecture} 3},
	language = {en},
	author = {Caulfield, Tristan and Pym, David},
	pages = {69},
}

@article{kairouz_advances_2019,
	title = {Advances and {Open} {Problems} in {Federated} {Learning}},
	url = {http://arxiv.org/abs/1912.04977},
	abstract = {Federated learning (FL) is a machine learning setting where many clients (e.g. mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g. service provider), while keeping the training data decentralized. FL embodies the principles of focused data collection and minimization, and can mitigate many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches. Motivated by the explosive growth in FL research, this paper discusses recent advances and presents an extensive collection of open problems and challenges.},
	language = {en},
	urldate = {2020-10-22},
	journal = {arXiv:1912.04977 [cs, stat]},
	author = {Kairouz, Peter and McMahan, H. Brendan and Avent, Brendan and Bellet, Aurélien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Keith and Charles, Zachary and Cormode, Graham and Cummings, Rachel and D'Oliveira, Rafael G. L. and Rouayheb, Salim El and Evans, David and Gardner, Josh and Garrett, Zachary and Gascón, Adrià and Ghazi, Badih and Gibbons, Phillip B. and Gruteser, Marco and Harchaoui, Zaid and He, Chaoyang and He, Lie and Huo, Zhouyuan and Hutchinson, Ben and Hsu, Justin and Jaggi, Martin and Javidi, Tara and Joshi, Gauri and Khodak, Mikhail and Konečný, Jakub and Korolova, Aleksandra and Koushanfar, Farinaz and Koyejo, Sanmi and Lepoint, Tancrède and Liu, Yang and Mittal, Prateek and Mohri, Mehryar and Nock, Richard and Özgür, Ayfer and Pagh, Rasmus and Raykova, Mariana and Qi, Hang and Ramage, Daniel and Raskar, Ramesh and Song, Dawn and Song, Weikang and Stich, Sebastian U. and Sun, Ziteng and Suresh, Ananda Theertha and Tramèr, Florian and Vepakomma, Praneeth and Wang, Jianyu and Xiong, Li and Xu, Zheng and Yang, Qiang and Yu, Felix X. and Yu, Han and Zhao, Sen},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.04977},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning, to read},
}

@article{abelson_keys_2015,
	title = {Keys under doormats: mandating insecurity by requiring government access to all data and communications},
	issn = {2057-2085, 2057-2093},
	shorttitle = {Keys under doormats},
	url = {https://academic.oup.com/cybersecurity/article-lookup/doi/10.1093/cybsec/tyv009},
	doi = {10.1093/cybsec/tyv009},
	abstract = {Twenty years ago, law enforcement organizations lobbied to require data and communication services to engineer their products to guarantee law enforcement access to all data. After lengthy debate and vigorous predictions of enforcement channels “going dark,” these attempts to regulate security technologies on the emerging Internet were abandoned. In the intervening years, innovation on the Internet ﬂourished, and law enforcement agencies found new and more effective means of accessing vastly larger quantities of data. Today, there are again calls for regulation to mandate the provision of exceptional access mechanisms. In this article, a group of computer scientists and security experts, many of whom participated in a 1997 study of these same topics, has convened to explore the likely effects of imposing extraordinary access mandates.},
	language = {en},
	urldate = {2020-10-20},
	journal = {Journal of Cybersecurity},
	author = {Abelson, Harold and Anderson, Ross and Bellovin, Steven M. and Benaloh, Josh and Blaze, Matt and Diffie, Whitfield and Gilmore, John and Green, Matthew and Landau, Susan and Neumann, Peter G. and Rivest, Ronald L. and Schiller, Jeffrey I. and Schneier, Bruce and Specter, Michael A. and Weitzner, Daniel J.},
	month = nov,
	year = {2015},
	keywords = {to read},
	pages = {tyv009},
}

@article{caulfield_philosophy_nodate-1,
	title = {Philosophy, {Politics}, and {Economics} of {Security} and {Privacy} - {Lecture} 2},
	language = {en},
	author = {Caulfield, Tristan and Pym, David},
	pages = {66},
}

@misc{noauthor_what_nodate,
	title = {What is {Differential} {Privacy}? – {A} {Few} {Thoughts} on {Cryptographic} {Engineering}},
	url = {https://blog.cryptographyengineering.com/2016/06/15/what-is-differential-privacy/},
	urldate = {2020-08-23},
	keywords = {to read},
}

@article{garfinkel_these_nodate,
	title = {These attacks on statistical databases are no longer a theoretical danger.},
	language = {en},
	author = {Garfinkel, Simson and Abowd, John M and Martindale, Christian},
	keywords = {to read},
	pages = {26},
}

@misc{noauthor_ted_nodate,
	title = {Ted on {Twitter}},
	url = {https://twitter.com/TedOnPrivacy/status/1151074940822675457},
	abstract = {@xchatty @HistDem This was really an excellent talk. I hope the video will be available later on! \#pets19 

For more information, the speaker gives two links:
https://t.co/sLzLRdHbdN
https://t.co/TupPO83FD8},
	language = {en-GB},
	urldate = {2020-10-14},
	journal = {Twitter},
	keywords = {to read},
}

@inproceedings{henry_formalizing_2011,
	address = {Oakland, CA, USA},
	title = {Formalizing {Anonymous} {Blacklisting} {Systems}},
	isbn = {978-1-4577-0147-4},
	url = {http://ieeexplore.ieee.org/document/5958023/},
	doi = {10.1109/SP.2011.13},
	abstract = {Anonymous communications networks, such as Tor, help to solve the real and important problem of enabling users to communicate privately over the Internet. However, in doing so, anonymous communications networks introduce an entirely new problem for the service providers—such as websites, IRC networks or mail servers—with which these users interact; in particular, since all anonymous users look alike, there is no way for the service providers to hold individual misbehaving anonymous users accountable for their actions. Recent research efforts have focused on using anonymous blacklisting systems (which are sometimes called anonymous revocation systems) to empower service providers with the ability to revoke access from abusive anonymous users. In contrast to revocable anonymity systems, which enable some trusted third party to deanonymize users, anonymous blacklisting systems provide users with a way to authenticate anonymously with a service provider, while enabling the service provider to revoke access from any users that misbehave, without revealing their identities. In this paper, we introduce the anonymous blacklisting problem and survey the literature on anonymous blacklisting systems, comparing and contrasting the architecture of various existing schemes, and discussing the tradeoffs inherent with each design. The literature on anonymous blacklisting systems lacks a uniﬁed set of deﬁnitions; each scheme operates under different trust assumptions and provides different security and privacy guarantees. Therefore, before we discuss the existing approaches in detail, we ﬁrst propose a formal deﬁnition for anonymous blacklisting systems, and a set of security and privacy properties that these systems should possess. We also outline a set of new performance requirements that anonymous blacklisting systems should satisfy to maximize their potential for real-world adoption, and give formal deﬁnitions for several optional features already supported by some schemes in the literature.},
	language = {en},
	urldate = {2020-01-29},
	booktitle = {2011 {IEEE} {Symposium} on {Security} and {Privacy}},
	publisher = {IEEE},
	author = {Henry, Ryan and Goldberg, Ian},
	month = may,
	year = {2011},
	keywords = {to read},
	pages = {81--95},
}

@misc{mervisjan_4_can_2019,
	title = {Can a set of equations keep {U}.{S}. census data private?},
	url = {https://www.sciencemag.org/news/2019/01/can-set-equations-keep-us-census-data-private},
	abstract = {Census Bureau embraces differential privacy in latest attempt to ensure confidentiality without sacrificing data quality},
	language = {en},
	urldate = {2020-10-14},
	journal = {Science {\textbar} AAAS},
	author = {MervisJan. 4, Jeffrey and {2019} and Pm, 2:50},
	month = jan,
	year = {2019},
	keywords = {to read},
}

@article{solove_taxonomy_nodate,
	title = {A taxonomy of privacy},
	volume = {154},
	language = {en},
	author = {Solove, Daniel},
	keywords = {to read},
	pages = {89},
}

@incollection{decew_privacy_2018,
	edition = {Spring 2018},
	title = {Privacy},
	url = {https://plato.stanford.edu/archives/spr2018/entries/privacy/},
	abstract = {The term “privacy” is used frequently in ordinarylanguage as well as in philosophical, political and legal discussions,yet there is no single definition or analysis or meaning of the term.The concept of privacy has broad historical roots in sociological andanthropological discussions about how extensively it is valued andpreserved in various cultures. Moreover, the concept has historicalorigins in well known philosophical discussions, most notablyAristotle’s distinction between the public sphere of political activityand the private sphere associated with family and domestic life. Yethistorical use of the term is not uniform, and there remains confusionover the meaning, value and scope of the concept of privacy., Early treatises on privacy appeared with the development of privacyprotection in American law from the 1890s onward, and privacyprotection was justified largely on moral grounds. This literaturehelps distinguish descriptive accounts of privacy, describingwhat is in fact protected as private, from normative accountsof privacy defending its value and the extent to which it should beprotected. In these discussions some treat privacy as aninterest with moral value, while others refer to it as a moralor legal right that ought to be protected by society or thelaw. Clearly one can be insensitive to another’s privacy interestswithout violating any right to privacy, if there is one., There are several skeptical and critical accounts of privacy.According to one well known argument there is no right to privacy andthere is nothing special about privacy, because any interest protectedas private can be equally well explained and protected by otherinterests or rights, most notably rights to property and bodilysecurity (Thomson, 1975). Other critiques argue that privacy interestsare not distinctive because the personal interests they protect areeconomically inefficient (Posner, 1981) or that they are not groundedin any adequate legal doctrine (Bork, 1990). Finally, there is thefeminist critique of privacy, that granting special status to privacyis detrimental to women and others because it is used as a shield todominate and control them, silence them, and cover up abuse (MacKinnon,1989)., Nevertheless, most theorists take the view that privacy is ameaningful and valuable concept. Philosophical debates concerningdefinitions of privacy became prominent in the second half of thetwentieth century, and are deeply affected by the development ofprivacy protection in the law. Some defend privacy as focusing oncontrol over information about oneself (Parent, 1983), while othersdefend it as a broader concept required for human dignity (Bloustein,1964), or crucial for intimacy (Gerstein, 1978; Inness, 1992). Othercommentators defend privacy as necessary for the development of variedand meaningful interpersonal relationships (Fried, 1970, Rachels,1975), or as the value that accords us the ability to control theaccess others have to us (Gavison, 1980; Allen, 1988; Moore, 2003), oras a set of norms necessary not only to control access but also toenhance personal expression and choice (Schoeman, 1992), or somecombination of these (DeCew, 1997). Discussion of the concept iscomplicated by the fact that privacy appears to be something we valueto provide a sphere within which we can be free from interference byothers, and yet it also appears to function negatively, as the cloakunder which one can hide domination, degradation, or physical harm towomen and others., This essay will discuss all of these topics, namely, (1) thehistorical roots of the concept of privacy, including the developmentof privacy protection in tort and constitutional law, and thephilosophical responses that privacy is merely reducible to otherinterests or is a coherent concept with fundamental value, (2) thecritiques of privacy as a right, (3) the wide array of philosophicaldefinitions or defenses of privacy as a concept, providing alternativeviews on the meaning and value of privacy (and whether or not it isculturally relative), as well as (4) the challenges to privacy posedin an age of technological advance. Overall, most writers defend thevalue of privacy protection despite the difficulties inherent in itsdefinition and its potential use to shield abuse. A contemporarycollection of essays on privacy provides strong evidence to supportthis point (Paul et al., 2000). The contributing authorsexamine various aspects of the right to privacy and its role in moralphilosophy, legal theory, and public policy. They also addressjustifications and foundational arguments for privacy rights.},
	urldate = {2020-10-19},
	booktitle = {The {Stanford} {Encyclopedia} of {Philosophy}},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {DeCew, Judith},
	editor = {Zalta, Edward N.},
	year = {2018},
	keywords = {to read},
}

@book{domingo-ferrer_privacy_2014,
	address = {Heraklion},
	title = {Privacy and data protection by design - from policy to engineering.},
	isbn = {978-92-9204-108-3},
	url = {http://bookshop.europa.eu/uri?target=EUB:NOTICE:TP0514111:EN:HTML},
	abstract = {This report shall promote the discussion on how privacy by design can be implemented with the help of engineering methods. It provides a basis for better understanding of the current state of the art concerning privacy by design with a focus on the technological side. Data protection authorities can use the report as a reference of currently available technologies and methods. Lastly, the report should help regulators to better understand the opportunities, challenges and limits of the by-design principles with respect to privacy and data protection, to improve the expressiveness and effective-ness of future policy.},
	language = {en},
	urldate = {2020-01-23},
	publisher = {ENISA},
	author = {Domingo-Ferrer, Josep and Hansen, Marit and Hoepman, Jaap-Henk and Le Métayer, Daniel and Tirtea, Rodica and Schiffner, Stefan and Danezis, George and {European Union} and {European Network and Information Security Agency}},
	year = {2014},
	note = {OCLC: 903502987},
	keywords = {to read},
}

@article{swanson_public_nodate,
	title = {The {Public} and the {Private} in {Aristotle}’s {Political} {Philosophy}},
	language = {en},
	author = {Swanson, Judith A},
	pages = {261},
}

@article{bernstein_understanding_nodate,
	title = {Understanding brute force},
	language = {en},
	author = {Bernstein, Daniel J},
	pages = {10},
}

@article{caulfield_philosophy_nodate-2,
	title = {Philosophy, {Politics}, and {Economics} of {Security} and {Privacy} - {Lecture} 1},
	language = {en},
	author = {Caulfield, Tristan and Pym, David},
	pages = {76},
}

@article{dwork_algorithmic_2013,
	title = {The {Algorithmic} {Foundations} of {Differential} {Privacy}},
	volume = {9},
	issn = {1551-305X, 1551-3068},
	url = {http://www.nowpublishers.com/articles/foundations-and-trends-in-theoretical-computer-science/TCS-042},
	doi = {10.1561/0400000042},
	language = {en},
	number = {3-4},
	urldate = {2020-09-26},
	journal = {Foundations and Trends® in Theoretical Computer Science},
	author = {Dwork, Cynthia and Roth, Aaron},
	year = {2013},
	pages = {211--407},
}

@misc{ross_anderson_sev3-xbib-7seppdf_nodate,
	title = {{SEv3}-xbib-7sep.pdf},
	author = {{Ross Anderson}},
}

@misc{ross_anderson_sev3-ch29-7seppdf_nodate,
	title = {{SEv3}-ch29-7sep.pdf},
	author = {{Ross Anderson}},
}

@misc{ross_anderson_sev3-ch28-7seppdf_nodate,
	title = {{SEv3}-ch28-7sep.pdf},
	author = {{Ross Anderson}},
}

@misc{ross_anderson_sev3-ch27-7seppdf_nodate,
	title = {{SEv3}-ch27-7sep.pdf},
	author = {{Ross Anderson}},
}

@misc{ross_anderson_sev3-ch25-7seppdf_nodate,
	title = {{SEv3}-ch25-7sep.pdf},
	author = {{Ross Anderson}},
}

@misc{ross_anderson_sev3-ch24-7seppdf_nodate,
	title = {{SEv3}-ch24-7sep.pdf},
	author = {{Ross Anderson}},
}

@misc{ross_anderson_sev3-ch26-7seppdf_nodate,
	title = {{SEv3}-ch26-7sep.pdf},
	author = {{Ross Anderson}},
}

@misc{ross_anderson_sev3-ch22-7seppdf_nodate,
	title = {{SEv3}-ch22-7sep.pdf},
	author = {{Ross Anderson}},
}

@misc{ross_anderson_sev3-ch23-7seppdf_nodate,
	title = {{SEv3}-ch23-7sep.pdf},
	author = {{Ross Anderson}},
}

@misc{ross_anderson_sev3-ch21-7seppdf_nodate,
	title = {{SEv3}-ch21-7sep.pdf},
	author = {{Ross Anderson}},
}

@misc{ross_anderson_sev3-ch19-7seppdf_nodate,
	title = {{SEv3}-ch19-7sep.pdf},
	author = {{Ross Anderson}},
}

@misc{ross_anderson_sev3-ch20-7seppdf_nodate,
	title = {{SEv3}-ch20-7sep.pdf},
	author = {{Ross Anderson}},
}

@misc{ross_anderson_sev3-ch18-7seppdf_nodate,
	title = {{SEv3}-ch18-7sep.pdf},
	author = {{Ross Anderson}},
}

@misc{ross_anderson_sev3-ch17-7seppdf_nodate,
	title = {{SEv3}-ch17-7sep.pdf},
	author = {{Ross Anderson}},
}

@misc{ross_anderson_sev3-ch15-7seppdf_nodate,
	title = {{SEv3}-ch15-7sep.pdf},
	author = {{Ross Anderson}},
}

@misc{ross_anderson_sev3-ch16-7seppdf_nodate,
	title = {{SEv3}-ch16-7sep.pdf},
	author = {{Ross Anderson}},
}

@misc{ross_anderson_sev3-ch13-7seppdf_nodate,
	title = {{SEv3}-ch13-7sep.pdf},
	author = {{Ross Anderson}},
}

@misc{ross_anderson_sev3-ch11-7seppdf_nodate,
	title = {{SEv3}-ch11-7sep.pdf},
	author = {{Ross Anderson}},
}

@misc{ross_anderson_sev3-ch14-7seppdf_nodate,
	title = {{SEv3}-ch14-7sep.pdf},
	author = {{Ross Anderson}},
}

@misc{ross_anderson_sev3-ch12-7seppdf_nodate,
	title = {{SEv3}-ch12-7sep.pdf},
	author = {{Ross Anderson}},
}

@misc{ross_anderson_sev3-ch10-7seppdf_nodate,
	title = {{SEv3}-ch10-7sep.pdf},
	author = {{Ross Anderson}},
}

@misc{ross_anderson_sev3-ch09-7seppdf_nodate,
	title = {{SEv3}-ch09-7sep.pdf},
	author = {{Ross Anderson}},
}

@misc{ross_anderson_sev3-ch08-7seppdf_nodate,
	title = {{SEv3}-ch08-7sep.pdf},
	author = {{Ross Anderson}},
}

@misc{ross_anderson_sev3-ch07-7seppdf_nodate,
	title = {{SEv3}-ch07-7sep.pdf},
	author = {{Ross Anderson}},
}

@misc{ross_anderson_sev3-ch06-7seppdf_nodate,
	title = {{SEv3}-ch06-7sep.pdf},
	author = {{Ross Anderson}},
}

@misc{ross_anderson_sev3-ch05-7seppdf_nodate,
	title = {{SEv3}-ch05-7sep.pdf},
	author = {{Ross Anderson}},
}

@misc{ross_anderson_sev3-ch04-7seppdf_nodate,
	title = {{SEv3}-ch04-7sep.pdf},
	author = {{Ross Anderson}},
}

@misc{ross_anderson_sev3-ch03-7seppdf_nodate,
	title = {{SEv3}-ch03-7sep.pdf},
	author = {{Ross Anderson}},
}

@misc{ross_anderson_sev3-ch02-7seppdf_nodate,
	title = {{SEv3}-ch02-7sep.pdf},
	author = {{Ross Anderson}},
}

@misc{ross_anderson_sev3-ch01-7seppdf_nodate,
	title = {{SEv3}-ch01-7sep.pdf},
	author = {{Ross Anderson}},
}

@misc{ross_anderson_preface_nodate,
	title = {Preface to the {Third} {Edition}},
	author = {{Ross Anderson}},
}

@misc{butler_improved_2016,
	title = {Improved {Authentication} for {Email} {Encryption} and {Security}},
	url = {https://protonmail.com/blog/encrypted_email_authentication/},
	abstract = {Today, we are happy to become the first and only secure email service to support Secure Remote Password (SRP), a much more secure authentication method.},
	language = {en-US},
	urldate = {2020-08-23},
	journal = {ProtonMail Blog},
	author = {Butler, Bart},
	month = dec,
	year = {2016},
	note = {Section: Encryption},
	keywords = {PGP},
}

@article{saltzer_protection_1975,
	title = {The protection of information in computer systems},
	volume = {63},
	issn = {0018-9219},
	url = {http://ieeexplore.ieee.org/document/1451869/},
	doi = {10.1109/PROC.1975.9939},
	abstract = {This tutorial paper explores the mechanics of protecting computer-stored information from unauthorized use or modification. It concentrates on those architectural structures--whether hardware or software--that are necessary to support information protection. The paper develops in three main sections. Section I describes desired functions, design principles, and examples of elementary protection and authentication mechanisms. Any reader familiar with computers should find the first section to be reasonably accessible. Section II requires some familiarity with descriptor-based computer architecture. It examines in depth the principles of modern protection architectures and the relation between capability systems and access control list systems, and ends with a brief analysis of protected subsystems and protected objects. The reader who is dismayed by either the prerequisites or the level of detail in the second section may wish to skip to Section III, which reviews the state of the art and current research projects and provides suggestions for further reading.},
	language = {en},
	number = {9},
	urldate = {2020-08-22},
	journal = {Proceedings of the IEEE},
	author = {Saltzer, J.H. and Schroeder, M.D.},
	year = {1975},
	pages = {1278--1308},
}

@article{wood_differential_2018,
	title = {Differential {Privacy}: {A} {Primer} for a {Non}-{Technical} {Audience}},
	issn = {1556-5068},
	shorttitle = {Differential {Privacy}},
	url = {https://www.ssrn.com/abstract=3338027},
	doi = {10.2139/ssrn.3338027},
	language = {en},
	urldate = {2020-02-29},
	journal = {SSRN Electronic Journal},
	author = {Wood, Alexandra and Altman, Micah and Bembenek, Aaron and Bun, Mark and Gaboardi, Marco and Honaker, James and Nissim, Kobbi and O'Brien, David and Steinke, Thomas and Vadhan, Salil},
	year = {2018},
}

@article{grossklags_when_nodate,
	title = {When 25 {Cents} is too much: {An} {Experiment} on {Willingness}-{To}-{Sell} and {Willingness}-{To}-{Protect} {Personal} {Information}},
	abstract = {We present an empirical study of willingness-to-pay for protecting information (we term it willingness-to-protect) and willingness-to-accept a proposal to sell information (willingness-to-accept). We conducted the study in two parts. In the first part we presented the study participants with two yes/no offers, asking them to protect themselves against information release for a fixed amount of money, or offering them a fixed amount of money to release their information. In the second part we asked subjects to specify their maximum willingness-to-protect for personal data and their minimum willingness-to-accept for the release of personal information.},
	language = {en},
	author = {Grossklags, Jens and Acquisti, Alessandro},
	pages = {22},
}

@article{rocher_estimating_2019,
	title = {Estimating the success of re-identifications in incomplete datasets using generative models},
	volume = {10},
	issn = {2041-1723},
	url = {http://www.nature.com/articles/s41467-019-10933-3},
	doi = {10.1038/s41467-019-10933-3},
	language = {en},
	number = {1},
	urldate = {2020-02-11},
	journal = {Nature Communications},
	author = {Rocher, Luc and Hendrickx, Julien M. and de Montjoye, Yves-Alexandre},
	month = dec,
	year = {2019},
	keywords = {to read},
	pages = {3069},
}

@book{stallings_cryptography_nodate,
	edition = {5th Edition},
	title = {Cryptography and {Network} {Security}: {Principles} and {Practice}},
	language = {en},
	author = {Stallings, William},
}

@book{katz_introduction_2007,
	edition = {2},
	title = {Introduction to {Modern} {Cryptography}: {Principles} and {Protocols}},
	isbn = {978-1-4200-1075-6},
	shorttitle = {Introduction to {Modern} {Cryptography}},
	url = {https://www.taylorfrancis.com/books/9781420010756},
	language = {en},
	urldate = {2020-01-23},
	publisher = {Chapman and Hall/CRC},
	author = {Katz, Jonathan and Lindell, Yehuda},
	month = aug,
	year = {2007},
	doi = {10.1201/9781420010756},
}

@book{gollmann_computer_2011,
	edition = {3},
	title = {Computer {Security}},
	language = {en},
	author = {Gollmann, Dieter},
	year = {2011},
}

@article{keshav_how_nodate,
	title = {How to {Read} a {Paper}},
	author = {Keshav, S},
}
